[
  {
    "objectID": "lectures/wyklad1.html",
    "href": "lectures/wyklad1.html",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "",
    "text": "Nature isn’t classical, dammit, and if you want to make a simulation of Nature, you’d better make it quantum mechanical, and by golly it’s a wonderful problem because it doesn’t look so easy.\nRichard Feynman\nCelem tego wykładu jest zrozumienie, czym są:\nQuntum Machine Learning",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#uczenie-maszynowe",
    "href": "lectures/wyklad1.html#uczenie-maszynowe",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "Uczenie maszynowe",
    "text": "Uczenie maszynowe\nUczenie maszynowe (ale również AI i uczenie głębokie) to nauka i ,,sztuka’’ opisująca jak sprawić by komputery mogły ,,uczyć się’’ na podstawie danych, tak by rozwiązać problemy, których typowe programowanie nie miałoby sensu (lub byłoby zbyt skomplikowane).\n\nIn 1959, Arthur Samuel:\na field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nDane\n\n\n\nModele\n\n\nUczenie nadzorowane (ang. supervised learning) - posiadając oznaczone dane \\((x_i, y_i)\\) szukamy funkcji \\(f(x_i) = y_i\\) tak by uogólnić ją na nowe dane. Np. dać kredyt, czy na obrazie jest kot albo pies itp.\nUczenie nienadzorowane (ang. unsupervised learning) - posiadając dane \\((x_i)\\) szukamy ukrytych struktur w danych.\nUczenie przez wzmacnianie (ang. reinforcement learning) - agent uczy się realizować zadania w środowisku na podstawie nagród i kar.\n\nJednym z podstawowych celów uczenia maszynowego (a takze i głębokiego), jest przypisanie klasy (target, labels) dla nowych, nieoznakowanych danych.\nIstnieją dwa główne typy dla tego zadania:\n\nRegresja – przewidywanie wartości ciągłej,\nklasyfikacja - przewidywanie wartości dyskretnej.\n\n\n\nFunkcja straty\nFunkcja straty mierzy jak przewidywania modelu są oddalone od rzeczywistych wartości.\n\nPomaga optymalizować parametry modelu przez mechanizm propagacji wstecz\nPozwala na dopasowanie modelu do danych\nMniejsza wartość funkcji straty = lepsza jakość modelu.\n\n\n\n\n\nSieci Neuronowe\n\n\nModel z parametrami do trenowania \\(f(x;\\theta)= \\sigma(Wx+b)\\) gdzie \\(\\theta= \\{W, b\\}\\)\nFunkcja kosztu \\(C = \\sum_{i} ( f(x_i, \\theta)-y_i )^2\\)\nSpadek po gradiencie\n\noblicz gradient funkcji kosztu\nzaktualizuj parametry \\(\\theta^{t+1} = \\theta^{t} - \\eta \\nabla C\\)\n\n\n\nimport torch\nfrom torch.autograd import Variable\n\ndata = torch.tensor([(0. , 1.), (0.1 , 1.1), (0.2 , 1.2)])\n\ndef model(phi, x=None):\n    return x*phi\n\ndef loss(a, b):\n    return torch.abs(a-b) ** 2\n\ndef avg_loss(phi):\n    c = 0 \n    for x, y in data:\n        c += loss(model(phi, x=x), y)\n    return c\n\nphi_ = Variable(torch.tensor(0.1), requires_grad=True)\nopt = torch.optim.Adam([phi_],lr=0.2)\n\nfor i in range(5):\n    l = avg_loss(phi_)\n    print(f\"cost: {l}, for phi: {phi_}\")\n    l.backward()\n    opt.step()\n\ncost: 3.5805001258850098, for phi: 0.10000000149011612\ncost: 3.44450044631958, for phi: 0.29999998211860657\ncost: 3.3168272972106934, for phi: 0.49334585666656494\ncost: 3.1936793327331543, for phi: 0.6854467988014221\ncost: 3.073840856552124, for phi: 0.878169059753418\n\n\n\n\nObliczenia kwantowe opisują przetwarzenie informacji na urządzeniach pracujących zgodnie z zasadami mechaniki kwantowej.\n\nUwaga! klasyczne komputery (tranzystory) również działają zgodnie z zasadami mechaniki kwantowej, ale wykonywane operacje opierają się o logikę klasyczną.\n\nOba kierunki są istotne w procesie przetwarzania danych obecnie i w niedalekiej przyszłości. Dlatego naturalnym pytaniem jest jak je ze sobą połączyć?\nQML to realizowanie metod uczenia maszynowego, które mogą być wykonywane na komputerach kwantowych.\nKwantowe uczenie maszynowe możemy określić jako uczenie maszynowe realizowane na komputerach kwantowych. Zasadniczym jest pytanie na ile i czy wogóle komputery kwantowe mogą poprawić jakość modeli uczenia maszynowego i czy pozwalają zrealizować coś więcej niż wykorzystanie klasycznych komputerów.\n\n\nHistoria MK\nPoczątek Mechaniki Kwantowej związane są z pracami Maxa Plancka (1900) i Alberta Einsteina (1905), którzy wprowadzili pojęcie kwantu - czyli najmniejszej porcji energii. Dalszy rozwój Mechaniki Kwantowej związany jest z badaniami takich naukowców jak Niels Bohr, Erwin Schrödinger, Louis de Broglie, Heisenberg, Dirac, Feynman i wielu innych. Pozostałe informacje możesz znaleźć w artykule o obliczeniach kwantowych\n\nInformatyków (najczęściej) nie interesuje, w jaki sposób właściwości fizyczne układów są wykorzystywane do przechowywania informacji w komputerze klasycznym. Podobnie, nie muszą się zastanawiać nad fizycznym mechanizmem, za pomocą którego informacja kwantowa jest realizowana w komputerze kwantowym. Czy prowadząc samochód zastanawiasz się, jak dokładnie działają wszystkie jego części? A pisząc kod modelu, zastanawiasz się, jak został on zaimplementowany w bibliotece?” Informatycy często nie muszą zagłębiać się w szczegóły fizycznej realizacji, skupiając się za to na wydajnym wykorzystaniu technologii komputerowych.\n\n\n\nHistoria obliczeń kwantowych\n\n1936 Alan Turing opublikował pracę On Computable Numbers, która stanowiła istotny krok w kierunku teoretycznych podstaw obliczeń (Hilbert Problems) - universal computing machine local\n1976 Roman S. Ingarden - Quantum Information Theory Roman S. Ingarden wprowadził pojęcie teorii informacji kwantowej, co miało kluczowe znaczenie dla rozwoju komputerów kwantowych.\n1980 Paul Benioff - Paul Benioff przedstawił teoretyczną koncepcję komputerów kwantowych jako fizycznych systemów, otwierając drzwi do praktycznych implementacji.\n1981 Richard Feynman - zwrócił uwagę na to, że klasyczne komputery nie są w stanie efektywnie symulować procesów kwantowych.\n1985 David Deutsch opracował pierwszy opis kwantowej maszyny Turinga i algorytmów przeznaczonych do uruchamiania na komputerach kwantowych, w tym bramek kwantowych.\n1994 Peter Shor opracował algorytm faktoryzacji liczb w czasie wielomianowym, co miało znaczenie dla kryptografii i bezpieczeństwa informacji.\n1996 Lov Grover - Lov Grover stworzył algorytm Grover’a, który okazał się wyjątkowo efektywny w przeszukiwaniu stanów kwantowych.\n2000 Został zbudowany pierwszy komputer kwantowy (5 qubitów) oparty na nuklearnym rezonansie magnetycznym, co stanowiło ważny krok w rozwoju fizycznych platform komputerów kwantowych.\n2001 Demonstracja algorytmu Shora potwierdziła praktyczność i znaczenie algorytmów kwantowych.\n2007 Firma D-Wave dokonała pierwszej sprzedaży komercyjnego komputera kwantowego, co miało wpływ na rozwój technologii komputerów kwantowych w sektorze prywatnym.\nFirma IBM dokonała znaczącego przełomu, pokazując, że klasyczne superkomputery nie są w stanie efektywnie symulować systemów zawierających więcej niż 56 kubitów, co jest znane jako “quantum supremacy.”\n23 października 2019: Google ogłosił uzyskanie tzw. quantum supremacy na 53 kubitach.\n2020 Zespół Jian-Wei Pana z University of Science and Technology of China dokonał przełomu, realizując 76 fotonowych kubitów na komputerze Jiuzhang.\n2023 Pierwszy logiczny qubit?\n2025 Google Quantum Echoes\n\nOd około 1990 roku fizycy i informatycy pracują nad fizyczną realizacją komputerów kwantowych. Jednym z popularnych modeli obliczeń na komputerach kwantowych jest model oparty na kwantowych obwodach (ang. quantum circuit), który wykorzystuje qubity zamiast klasycznych bitów. Podobnie jak w przypadku obwodów klasycznych, w modelu kwantowym definiuje się bramki kwantowe (ang. quantum gates), które pozwalają na wykonywanie operacji na qubitach.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#dlaczego-chcemy-używać-komputerów-kwantowych",
    "href": "lectures/wyklad1.html#dlaczego-chcemy-używać-komputerów-kwantowych",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "Dlaczego chcemy używać komputerów kwantowych?",
    "text": "Dlaczego chcemy używać komputerów kwantowych?\n\nKwantowa złożoność (Quantum Complexity)\nNowy paradygmat wykorzystuje unikalne cechy interferencji, superpozycji i splątania w celu wykonywania obliczeń.\nObecnie realizowany jest w trzech modelach:\n\nQuantum Circuits (Obwody Kwantowe) - oparty na modelu bramkowym, pozwala realizować algorytmy typu QAOA, VQA, oraz metody hybrydowe.\nAdiabatyczne Obliczenia Kwantowe (D-Wave) - polegające na minimalizacji energii, z wykorzystaniem optymalizacji QUBO i analogii do modelu Isinga.\nTopologiczne Komputery Kwantowe - oparte na topologicznych kubitach.\n\nProblemy, które uważamy za trudne do rozwiązania klasycznie, takie jak optymalizacja, stają się łatwiejsze dla komputerów kwantowych. Przykładem moze być faktoryzacja liczb. Klasyczne komputery nie są w stanie efektywnie symulować działania kwantowych komputerów. Koszt najlepszych symulatorów rośnie wykładniczo wraz z liczbą kubitów. Możliwości komputerów kwantowych są potencjalnie ogromne, ale obecnie istnieją pewne ograniczenia link. Kwantowy komputer może być używany do efektywnej symulacji niemal dowolnego procesu fizycznego zachodzącego w przyrodzie, choć nie zawsze jesteśmy pewni, czy taka symulacja jest możliwa.\nPodstawowym faktem przewagi komputerów kwantowych nad klasycznymi jest tzw. parallelizm. Ze względu, iż kubity moga znajdowac się w superpozycji stanów, komputer kwantowy może przeprowadzic obliczenia jednocześnie na wszystkich stanach. Co dokładnie to oznacza, poznamy w dalszej czesci wykładu. Rozważmy sytuację w której chcemy poznac działanie funkcji \\(f(x)\\) dla pewnego argumentu \\(x\\) (dla pewnej liczby). Aby znaleźc wynik dla dwóch liczb (np. \\(x=0\\) i \\(x=1\\)) klasyczny komputer musi wykonac dwie operacje. Komputer kwantowy może uzuskac ten wynik przeprowadzajac obliczenia jednocześnie dla obu warości. Do wykonania takiej operacji wystarczy jeden kubit. Następnie jeżeli będziemy chcieli obliczyc nasza funkcję dla kolejnych liczb \\(x=2\\) (która binarnie reprezentowana jest jako \\(10\\)) oraz liczby \\(x=3\\) (binarnie \\(11\\)) musimy dodac kolejny (jeden!) kubit. Dwa kubity moga posłużyc do realizacji czterech równoległych operacji. Jeśli rozważymy 3 kubity znowu mozemy podwoic ilośc operacji (3 kubity maja 8 stanów bazowych). Dodanie kubitu do komputera kwantowego pozwala podwoic liczbę obliczeń. W przypadku klasycznego komputera aby uzyskac taki efekt, potrzeba podwoic rownież liczbę bitów. n-kubitów moze realizowac \\(2^n\\) równoległych obliczeń.\n\n\nKwantowa korekcja błędów (Quantum Error Correction)\nDekoherencja, czyli oddziaływanie z otoczeniem, niszczy stan komputera kwantowego i wprowadza błędy obliczeniowe. Istnieje potrzeba zabezpieczenia przed tym zjawiskiem. Obliczenia kwantowe wymagają tzw. korekcji błędów, która pomaga w utrzymaniu integralności obliczeń na komputerach kwantowych. Aktualnie mówimy o erze Noisy Intermediate-Scale Quantum (NISQ), co oznacza, że komputery kwantowe wciąż potrzebują rozwoju w zakresie korekcji błędów i stabilności.\n\n\nRealizacja fizyczna komputerów kwantowych\nprocesory kwantowe\n\n\n\nProces obliczeń kwantowych\nWykonanie obliczeń związane jest z pojęciem fizycznego doświadczenia. Będzie się ono składać z trzech części:\n\nprzygotowanie (przygotuj stan kwantowy kubitów),\newolucja (przeprowadź transformację za pomocą bramek kwantowych),\npomiar i interpretacja wyników.\n\n\nPodobnie w informatyce i w analizach danych wykonujemy obliczenia klasyczne. przygotowujemy dane (stan początkowy); następnie wykonujemy program (ewolucja) i odczytujemy wyniki (pomiar).\n\nNie obserwujemy tych etapów podczas codziennej interakcji z komputerem, więc nie zauważamy w sposób świadomy powyższego schematu działania. Piotr Gawron, Oscar Słowik - Rewolucja Stanu, Fantastyczne wprowadzenie do informatyki kwantowej.\n\nKażdy komputer kwantowy (koprocesor) musi komunikować się z podukładem klasycznym.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#quantum-machine-learning",
    "href": "lectures/wyklad1.html#quantum-machine-learning",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "Quantum Machine Learning",
    "text": "Quantum Machine Learning\n\nKlasyczne dane w kwantowym uczeniu maszynowym\n\n\nCC - Classical data using classical computers, algorytmy inspirowane obliczeniami kwantowymi\nQC - Quantum data using classical (ML) computers. link1, link2, link3\nCQ - Classical data on qunatum computers. Na tym chcemy się skupić.\nQQ - Quantum data on quantum computers. Who knows?\n\n\n\nDostęp do obliczeń kwantowych w chmurze\n\nIBM Quantum z wykorzystaniem biblioteki qiskit.\nPennylane z wykorzystaniem biblioteki pennylane.\nCirq Google z wykorzystaniem biblioteki cirq.\nD-Wave - Python\nXanadu - Pennylane Python library\nAmazon braket - AWS Python, Julia",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html",
    "href": "lectures/wyklad2.html",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów",
    "section": "",
    "text": "\\[\n\\newcommand{\\bra}[1]{\\left \\langle #1 \\right \\rvert}\n\\newcommand{\\ket}[1]{\\left \\rvert #1 \\right \\rangle}\n\\newcommand{\\braket}[2]{\\left \\langle #1 \\middle \\rvert #2 \\right \\rangle}\n\\]\nMechanika Kwantowa opiera się na algebrze liniowej.\nW ogólności teoria ta posługuje się pojęciem nieskończenie wymiarowej przestrzeni liniowej. Na szczęście do opisu kubitów (2-dim) i układów kwantowych (\\(2^{n}\\)-dim) wystarczy nam pojęcie skończenie wymiarowej przestrzeni wektorowej. Bardzo upraszcza nam to naukę o kwantowym uczeniu maszynowym, gdyż wiele problemów matematycznych (dla fizyków) tutaj nie występuje. Upraszcza to również ilość potrzebnych matematycznych pojęć.\nBędziemy posługiwali się notacją Diraca, jednego z twórców mechaniki kwantowej. W książce Ch. Bernhardta “Obliczenia kwantowe dla każdego” autor rezygnuje z liczb zespolonych, na rzecz liczb rzeczywistych. O ile podejście takie sprawdza się na poziomie opisu o tyle dla pełnego zrozumienia posługiwanie się liczbami zespolonymi jest niezbędne.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#liczby-rzeczywiste-i-zespolone---przypomnienie",
    "href": "lectures/wyklad2.html#liczby-rzeczywiste-i-zespolone---przypomnienie",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów",
    "section": "Liczby rzeczywiste i zespolone - przypomnienie",
    "text": "Liczby rzeczywiste i zespolone - przypomnienie\nLiczby to matematyczne, abstrakcyjne pojęcia wywodzące się z teorii mnogości (zbiorów). Przykładowo, liczbę 42 można zapisa w postaci dziesiętnej lub binarnej \\(42=101010_2\\). Możemy znaleźć 42 przedmioty i je przeliczyć, ale w naszym przypadku skupimy się na abstrakcyjnym pojęciu liczby, niezależnie od jej reprezentacji. Liczba 42 jest liczbą naturalną. Zbiór liczb naturalnych oznaczamy jako \\(\\mathbb{N}\\). Identyczne cechy abstrakcji mają liczby całkowite \\(\\mathbb{Z}\\), liczby wymierne \\(\\mathbb{Q}\\), liczby rzeczywiste \\(\\mathbb{R}\\) oraz liczby zespolone \\(\\mathbb{C}\\). Nie możemy zobaczyć ani dotknąć liczb, ale możemy wykonywać na nich operacje matematyczne.\nWarto zaznaczyc,że liczby zespolone nie są bardziej abstrakcyjne niż liczby rzeczywiste, czy naturalne.\nLiczba zespolona (we współrzędnych Kartezjańskich) składa się z (dwóch liczb rzeczywistych) części rzeczywistej i urojonej: \\[z=x + i y\\] gdzie \\(i^2=-1\\).\nCzęść rzeczywista to \\(R(z)=x\\) i częśc urojona to \\(I(z)=y\\).\nNa przykład: \\[1+i\\sqrt{3}\\] \\(R(z)=1\\) i \\(I(z)=\\sqrt{3}\\).\nInaczej mówiąc, liczba zespolona jest sumą liczby rzeczywistej i urojonej.\nLiczy zespolone, można traktowac jako punkty na płaszczyźnie o współrzędnych \\(x\\) i \\(y\\).\n\nKażdą liczbę zespoloną możemy zapisać w postaci polarnej (współrzędne biegunowe)\n\\[ z=r\\, e^{i \\phi} , \\] gdzie \\(r=|z|\\) to moduł liczby zespolonej, a \\(\\phi\\) to jej argument czyli wyrażony w radianach kąt między osią rzeczywistą a półprostą poprowadzoną od środka ukł. wsp. i przechodzącą przez punkt \\(z\\).\n\\[ z = r\\, e^{i \\, \\phi} = r\\, (\\cos{\\phi} + i\\, \\sin{\\phi}) \\] gdzie: \\[ r = |z| = \\sqrt{x^2 + y^2} \\] \\[ \\phi = \\arctan{\\frac{y}{x}}. \\]\nNatomiast: \\[ x = r \\cos{\\phi} \\] \\[ y = r \\sin{\\phi} \\]\nDla naszego przykładu: \\[ 1+i\\sqrt{3} = 2 e^{i \\frac{\\pi}{3}} .\\]\n\nUdowodnij samodzielnie, że powyższe równanie jest prawdziwe.\n\nLiczby zespolone można dodawa, mnożyc i dzieli zgodnie z zwykłymi regułami arytmetyki. Dodawanie liczb zespolonych jest łatwe dla liczb w postaci kartezjańskiej. Natomiast mnożenie liczb zespolonych upraszcza się dla postaci biegunowej (następuje zamiana mnożenia na dodawanie fazy).\nLiczba sprzężona do liczby zespolonej powstaje poprzez zmianę znaku części urojonej\n\\[ z=x + i\\, y\\,\\,\\,\\,\\] to \\[\\,\\,\\,z^*=x - i y = r  e^{-i \\phi} .\\]\nNorma liczby zespolonej \\[ z=x + i y\\,\\,\\,\\,\\] to \\[\\,\\,\\,|z|=\\sqrt{x^2 + y^2}=r .\\]\nKwadrat normy liczby zespolonej \\(z=x + i y\\,\\,\\,\\,\\) to \\(\\,\\,\\, |z|^2=x^2 + y^2=r^2\\). Warto zauważyc, że każdy kwadrat modułu daje w wyniku nieujemną liczbę rzeczywistą.\nMożna go również zapisać jako \\[|z|^2=z z^* = z^* z\\]\nCzynniki fazowe to szczególna klasa liczb zespolonych \\(z\\) dla której \\(r=1\\).\nOtrzymujemy wtedy:\n\\[ z=e^{i \\phi}=\\cos{\\phi} + i\\, \\sin{\\phi} \\]\n\\[ z z^* = 1 \\]\n\nUdowodnij w kartezjańskim i polarnym układzie oniesienia.\n\n\nile wynosi \\(z_1 z_2\\)\n\n\nile wynosi \\(\\frac{z_1}{z_2}\\)",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#wektory-i-przestrzenie-wektorowe",
    "href": "lectures/wyklad2.html#wektory-i-przestrzenie-wektorowe",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów",
    "section": "Wektory i przestrzenie wektorowe",
    "text": "Wektory i przestrzenie wektorowe\nNiech dany będzie zbiór \\(\\mathbb{V}\\) oraz zbiór \\(\\mathbb{K}\\). Elementy zbioru \\(\\mathbb{V}\\) można ze sobą dodawać i mnożyć przez elementy zbioru \\(\\mathbb{K}\\). Wraz z dodatkowymi opracjami (zdefiniowanymi poniżej) zbiór ten będziemy nazwywali przestrzenią wektorową. Jej elementy to wektory ket \\(\\ket{u}\\) (lub kety).\nJeśli współczynniki liczbowe wektorów będą rzeczywiste to będziemy mówić o przestrzeni wektorowej rzeczywistej. Natomiast jeśli liczby te będą zespolone to będziemy mówić o przestrzeni wektorowej zespolonej.\nMyśląc o wektorach często wyobrażamy je sobie jako strzałki w przestrzeni. Przez strzałki rozumiemy tutaj obiekty znajdujące się w zwykłej przestrzeni i posiadające wielkoś oraz kierunek. Wektory takie mają trzy składowe - trzy (rzeczywiste) współrzędne przestrzenne.\nNa tych zajęciach lepiej zapomniec o tej koncepcji. Wszystkie wektory będą reprezentowane jako abstrakcyjne elementy przestrzeni wektorowej. Warto jednak pamiętać, że wszystkie własności (algebraiczne) wektorów są również spełnione dla strzałek.\n\nAksjomaty przestrzeni stanów\nNiech \\(\\ket{v}\\) , \\(\\ket{u}\\), \\(\\ket{z}\\) będą dowolnymi wektorami, natomiast \\(\\alpha\\) i \\(\\beta\\) dowolnymi liczbami.\n\nSuma dwóch wektorów ket jest wektorem ket \\[\\ket{v} + \\ket{u} = \\ket{z}\\]\nDodawanie wektorów jest przemienne: \\[\\ket{v} + \\ket{u} = \\ket{u} + \\ket{v}\\]\nDodawanie wektorów jest łączne: \\[\\ket{v} + (\\ket{u} + \\ket{z}) = (\\ket{v} + \\ket{u}) + \\ket{z}\\]\nIstnieje szczególny (i jedyny) wektor \\(\\ket{v}\\) odwrotny do wektora \\(\\ket{u}\\): \\[\\ket{v} + \\ket{u} = 0\\]\nIstnieje szczególny (i jedyny) wektor \\(0\\) zerowy. Dla każdego wektora \\(\\ket{v}\\) zachodzi: \\[\\ket{v} + 0 = 0 + \\ket{v} = \\ket{v}\\]\n1*wektor = wektor: \\[1 \\ket{v} = \\ket{v}\\]\nŁączność mnożenia przez skalar: \\[\\alpha (\\beta \\ket{v}) = (\\alpha \\beta) \\ket{v}\\]\nRozdzielność mnożenia przez skalar względem dodawania wektorów: \\[\\alpha (\\ket{v} + \\ket{u}) = \\alpha \\ket{v} + \\alpha \\ket{u}\\]\nRozdzielność dodawania skalarów względem mnożenia przez wektor: \\[(\\alpha + \\beta) \\ket{v} = \\alpha \\ket{v} + \\beta \\ket{v}\\]\n\n\n\nWektory kolumnowe\nZapiszmy pionową jednokolumnową tablicę liczb: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix} \\]\nMnożenie przez liczbę: \\[ \\alpha \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix} = \\begin{bmatrix} \\alpha x_1 \\\\ \\alpha x_2 \\\\ .\\\\ \\alpha x_n \\end{bmatrix} \\]\nDodawanie kolumn: \\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix} + \\begin{bmatrix} y_1 \\\\ y_2 \\\\ .\\\\ y_n \\end{bmatrix} = \\begin{bmatrix} x_1+y_1 \\\\ x_2+y_2 \\\\ .\\\\ x_n+y_n \\end{bmatrix}\\]\nPozwala to otrzymać konkretną reprezentację wektorów, które będziemy oznaczać w notacji Diraca przez “ket” \\(\\ket{.}\\).\n\n\nWektory wierszowe\n\\[ \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}\\]\nAnalogicznie do poprzedniego przykładu łatwo określić jak dodawać je ze sobą i mnożyć przez liczbę. W notacji Diraca będziemy takie wektory oznaczali przez “bra” \\(\\bra{.}\\).\n\n\nTranspozycja i sprzężenie Hermitowskie.\nTranspozycja \\(T\\) Zamienia wektory wierszowe na kolumnowe i odwrotnie.\n\\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}^{T} = \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}\\]\noraz \\[ \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}^{T} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}\\]\nNatomiast sprzężenie hermitowskie \\(\\dagger = T \\ast\\) dodatkowo do transpozycji dodaje sprzężenie zespolone.\n\\[\\ket{u}^{\\dagger} = \\bra{u}\\] \\[\\bra{u}^{\\dagger} = \\ket{u}\\]\nCzyli: \\[ (\\ket{u} + \\ket{v})^{\\dagger} = \\bra{u} + \\bra{v} \\] oraz \\[ \\alpha \\ket{u} \\to \\bra{u} \\alpha^*\\]\n\\[ \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}^{\\dagger} = \\begin{bmatrix} x_1^* \\,\\, x_2^* \\,\\, \\dots \\,\\, x_n^* \\end{bmatrix}\\]\noraz \\[ \\begin{bmatrix} x_1 \\,\\, x_2 \\,\\, \\dots \\,\\, x_n \\end{bmatrix}^{\\dagger} = \\begin{bmatrix} x_1^* \\\\ x_2^* \\\\ .\\\\ x_n^* \\end{bmatrix}\\]\n\n\nIloczyn skalarny\nIloczynem skalarnym dwóch wektorów \\(\\ket{u}\\) i \\(\\ket{v}\\) nazywany funkcję, która zwraca liczbę.\n\n\\(\\braket{u}{v} = \\braket{v}{u}^{\\ast}\\)\n\\((\\alpha \\bra{u})\\ket{v} = \\alpha \\braket{u}{v}\\)\n\\((\\bra{u} + \\bra{v}) \\ket{z} = \\braket{u}{z} +\\braket{v}{z}\\)\n\\(\\braket{u}{u} &gt; 0\\)\n\\(\\braket{u}{u} = 0, gdy \\ket{u}=\\ket{0}\\)\n\nDla dwóch wektorów \\(\\ket{u}\\) i \\(\\ket{v}\\) otrzymujemy: \\[ \\ket{u} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ .\\\\ x_n \\end{bmatrix}, \\ket{v} = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ .\\\\ y_n \\end{bmatrix} \\]\n\\[ \\braket{u}{v} = x_1^{*}y_1 +x_2^{*}y_2 + \\dots + x_n^{*}y_n\\]\n\nZadanie - Udowodnij, że \\(\\braket{u}{u}\\) jest liczbą rzeczywistą.\n\nwektor znormalizowany \\(\\braket{u}{u}=1\\)\nwektory ortogonalne \\(\\braket{u}{v}=0\\)\n\n\nKombinacja liniowa wektorów\nDla dwóch wektorów \\(\\ket{u}\\) i \\(\\ket{v}\\) oraz dwóch liczb \\(\\alpha\\), \\(\\beta\\) możemy stworzyć nowy wektor: \\[\\ket{z} = \\alpha \\ket{u} + \\beta \\ket{v}\\] Wektor ten nazywamy kombinacją liniową wektorów \\(\\ket{u}\\) i \\(\\ket{v}\\) o współczynnikach \\(\\alpha\\) i \\(\\beta\\).\n\n\nBaza\nKażda przestrzeń wektorowa ma bazę.\nDowolny wektor można zapisa jako kombinację liniową wektorów bazowych.\nInteresowac będzie nas baza (obliczeniowa) dla której:\n\\[ \\braket{e_i}{e_i}=1 \\,\\, \\braket{e_i}{e_j}=0 \\,\\, \\text{dla i} \\neq j \\] gdzie \\(i,\\,j = 1,2,\\dots, n\\).\nDowolny wektor \\(\\ket{u}\\) możemy zapisa jako: \\[ \\ket{u} = \\braket{e_1}{u}\\ket{e_1} + \\braket{e_2}{u}\\ket{e_2} + ... + \\braket{e_n}{u}\\ket{e_n}  \\]\nWarto zauważyc: \\[\\braket{e_1}{u}= x_1\\] \\[\\ket{u} = \\sum_{i=1}^{n} \\ket{i}\\bra{i} \\ket{u}\\]",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#formalizm-matematyczny-obliczeń-kwantowych",
    "href": "lectures/wyklad2.html#formalizm-matematyczny-obliczeń-kwantowych",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów",
    "section": "Formalizm matematyczny obliczeń kwantowych",
    "text": "Formalizm matematyczny obliczeń kwantowych\nTa wiedza wystarczy do wyjaśnienia notacji Diraca.\nIloczyn skalarny \\(\\braket{\\psi}{\\phi}\\) wektorów \\(\\ket{\\psi}\\) i \\(\\ket{\\phi}\\) czytamy jako braket u v.\n\nStan\nW fizyce klasycznej znajomość stanu układu oznacza, iż wiemy wszystko co jest potrzebne\nStanem w mechanice kwantowej nazywamy wektor:\n\\[\\ket{\\psi} = x_0 \\ket{0} + x_1 \\ket{1} + \\dots x_{n-1} \\ket{n-1}\\]\nChcemy aby współczynniki \\(x_i\\) były liczbami zespolonymi a cały wektor był unormowany do 1.\nLiczby \\(x_i\\) nazywamy amplitudami prawdopodobieństwa stanu kwantowego. Jeśli przynajmniej dwie liczby \\(x_i\\) są niezerowe, to układ znajduje się w superpozycji stanów.\n\n\nKubit\n\nElementarnym obiektem w informatyce kwantowej jest kubit, który realizowany jest jako dwu wymiarowy układ kwantowy. Stan kwantowy kubitu opisuje wektor w przestrzeni liniowej \\(\\mathbb{C}^2\\).\nW celu wykonywania obliczeń i opisu stanu kubitu wybierzemy tzw. bazę obliczeniową: \\[\\ket{0} = \\begin{bmatrix} 1 \\\\ 0  \\end{bmatrix} , \\ket{1} = \\begin{bmatrix} 0 \\\\ 1  \\end{bmatrix}\\]\nTo co wyróżnia kubit w porównaniu do klasycznego bitu dowolny stan \\(\\ket{\\psi}\\) może być superpozycją stanów bazowych: \\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1} = \\alpha \\begin{bmatrix} 1 \\\\ 0\\end{bmatrix} + \\beta \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\n\\] \\[\n\\bra{\\psi} = \\alpha^{*} \\bra{0} + \\beta^{*} \\bra{1} = \\alpha^{*} [1 \\,\\, 0] + \\beta^{*} [0 \\,\\, 1] =\n[\\alpha^{*}  \\, \\, \\beta^{*} ]\n\\]\ndla którego zachodzi warunek normalizacji: \\[\n\\braket{\\psi}{\\psi} = |\\alpha|^2 + |\\beta|^2 = 1\n\\] gdzie \\(\\alpha, \\beta \\in \\mathbb{C}\\).\n\nZADANIE - oblicz \\(\\braket{\\psi}{\\psi}\\).\n\nLiczby \\(\\alpha\\) i \\(\\beta\\) nazywamy amplitudami prawdopodobieństwa. Są one reprezentowane przez liczby zespolone. Potrzeba 4 liczb rzeczywistych aby je opisać. Ze względu na warunek normalizacji jedną liczbę można obliczyc co oznacza potrzebę użycia już tylko trzech liczb rzeczywiste.\nStan kubitu możemy zapisać w postaci: \\[\n\\ket{\\psi} = e^{i \\gamma}\\left( \\cos{\\frac{\\phi}{2}} \\ket{0} + e^{i \\theta} \\sin{\\frac{\\phi}{2}} \\ket{1} \\right)\n\\] gdzie \\(\\phi \\in [0, \\pi]\\), \\(\\theta \\in [0, 2\\pi]\\) i \\(\\gamma \\in [0, 2\\pi]\\) są liczbami rzeczywistymi.\nWspółczynnik \\(e^{i \\gamma}\\) nazywamy fazą globalną. Ze względu, iż analizować będziemy kwadraty amplitud prawdopodobieństwa to faza globalna nie ma znaczenia. Dlatego możemy napisać: \\[\n\\ket{\\psi} = \\cos{\\frac{\\phi}{2}} \\ket{0} + e^{i \\theta} \\sin{\\frac{\\phi}{2}} \\ket{1}\n= \\begin{bmatrix} \\cos{\\frac{\\phi}{2}} \\\\ e^{i \\theta} \\sin{\\frac{\\phi}{2}} \\end{bmatrix}\n\\]\nWarto zauważyć, że dwa dowolne stany kubitów \\(\\ket{\\psi}\\) i \\(\\ket{\\phi}\\) różnią się o czynnik fazowy \\(e^{i \\gamma}\\) to stany te dają identyczne wyniki.\nLiczby rzeczywiste \\(\\phi\\) i \\(\\theta\\) nazywamy kątami kubitu i możemy interpretować je jako współrzędne na sferze Blocha. Bardzo często będziemy wykorzystywać ją do wizualizacji stanów kubitów.\nStany w bazie obliczeniowej, którymi często będziemy operowac: \\[\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0} + \\ket{1})\\] \\[\\ket{-} = \\frac{1}{\\sqrt{2}}(\\ket{0} - \\ket{1})\\] \\[\\ket{i} =\\frac{1}{\\sqrt{2}}(\\ket{0} + i \\ket{1})\\] \\[\\ket{-i} =\\frac{1}{\\sqrt{2}}(\\ket{0} - i \\ket{1})\\]\nLub: \\[\\frac{1}{\\sqrt{2}}(\\ket{0} + e^{i\\pi/6} \\ket{1})\\] \\[\\frac{\\sqrt{3}}{2}(\\ket{0} + \\frac{1}{2} \\ket{1})\\]\n\nKubit może by dowolnym punktem na sferze Blocha.\n\n\n\nDwa kubity\nZłączenie układu dwóch kubitów realizowane jest przez iloczyn tensorowy (iloczyn Kroneckera).\nRozważmy dwa stany kubitów \\(\\ket{\\psi}\\), \\(\\ket{\\phi}\\)\n\\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1} = \\begin{bmatrix} \\alpha \\\\ \\beta \\end{bmatrix}\\, ,\\,\\,\n\\ket{\\phi} = \\gamma \\ket{0} + \\delta \\ket{1} = \\begin{bmatrix} \\gamma \\\\ \\delta \\end{bmatrix}\n\\]\nStan dwukubitowy: \\[\n\\ket{\\psi} \\otimes \\ket{\\phi} = \\begin{bmatrix} \\alpha \\gamma \\\\ \\alpha \\delta \\\\ \\beta \\gamma \\\\ \\beta \\delta \\end{bmatrix} = \\alpha \\gamma \\ket{0} \\otimes \\ket{0} + \\beta \\delta \\ket{1} \\otimes \\ket{0}  + \\alpha \\delta \\ket{0} \\otimes \\ket{1}  + \\beta \\delta \\ket{1} \\otimes \\ket{1}\n\\] co możemy zapisa jako: \\[\n\\ket{\\psi \\phi} = \\alpha \\gamma \\ket{00} + \\beta \\delta \\ket{10}  + \\alpha \\delta \\ket{01}  + \\beta \\delta \\ket{11}\n\\] gdzie: \\[\n\\ket{00} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\, \\,\n\\ket{01} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\, \\,\n\\ket{10} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\, \\,\n\\ket{11} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}\n\\]\nPo przenumerowaniu stanów możemy napisac: \\[\n\\ket{\\Phi} = c_0 \\ket{0} + c_1 \\ket{1}  + c_2 \\ket{2}  + c_3 \\ket{3}\n\\] dla którego: \\[\n|c_0|^2 + |c_1|^2 + |c_2|^2 + |c_3|^2 = 1\n\\]\n\n\nStan separowalny i splątany\nJeżeli istnieją stany \\(\\ket{\\phi_1}\\) i \\(\\ket{\\phi_2}\\) takie, że \\[\\ket{\\psi} = \\ket{\\phi_1} \\otimes \\ket{\\phi_2}\\] to stan nazywamy separowalny.\nZobaczmy, czy istnieje przypadek w którym stan układu dwóch kubitów nie da się zaprezentowac jako iloczynu tensorowego podukładów. Aby to sprawdzic zobaczmy czy istnieją takie liczby \\(c_0, c_1, c_2, c_3\\) dla których nie da się znaleźc \\(\\alpha, \\beta,\\gamma, \\delta\\), które spełniają układ równań: \\[c_0 = \\alpha \\gamma , \\, c_1 = \\alpha \\delta , \\, c_2 = \\beta \\gamma , \\, c_3 = \\beta \\delta \\]\nRozważmy stan \\[\\ket{bell} = \\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{3}) = \\frac{1}{\\sqrt{2}}(\\ket{00}+\\ket{11})\\]\nZałóżmy, że możemy zapisa stan bell w postaci: \\[ \\alpha \\gamma \\ket{0} + \\beta \\delta \\ket{1}  + \\alpha \\delta \\ket{2}  + \\beta \\delta \\ket{3} \\]\nAby stan bell był separowalny musi by spełniony układ równań:\n\\[\\begin{eqnarray}\n\\alpha \\gamma = \\frac{1}{\\sqrt{2}} \\\\ \\alpha \\delta = 0 \\\\ \\beta \\gamma = 0 \\\\ \\beta \\delta =\\frac{1}{\\sqrt{2}}\n\\end{eqnarray}\\]\nZ warunku drugiego mamy dwie możliwości: albo \\(\\alpha=0\\) lub \\(\\delta=0\\). Jeżeli \\(\\alpha=0\\) to warunek pierwszy nie może byc spełniony. Jeżeli \\(\\delta=0\\) to warunek czwarty nie może byc spełniony. Otrzymujemy sprzecznośc.\nProwadzi to do wniosu, że stan bell'a nie jest stanem separowalnym i jest stanem splątanym. Stany te mają bardzo nieintuicyjne własności. Związany jest z nimi słynny paradox EPR oraz tak zwane nierówności Bella.\n\nSplątane stany Bell’a, wraz z zasadą superpozycji będą podstawowymi kwantowymi własnościami pozwalającymi zrealizowac przewagę obliczeń kwantowych nad obliczeniami klasycznymi.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów"
    ]
  },
  {
    "objectID": "lectures/wyklad2.html#pomiar-w-bazie-z",
    "href": "lectures/wyklad2.html#pomiar-w-bazie-z",
    "title": "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów",
    "section": "Pomiar w bazie Z",
    "text": "Pomiar w bazie Z\nW opisie kubitów wybraliśmy specyficzą bazę (obliczeniową) wektorów, która rozkłada każdy wektor na kombinację wektora \\(\\ket{0}\\) i \\(\\ket{1}\\).\nZasady przestrzeni wektorowej i mechaniki kwantowej dopuszczają tworzenie kombinacji liniowej (superpozycji) dla tych dwóch stanów. \\[\n\\ket{\\psi} = \\alpha \\ket{0} + \\beta \\ket{1}\n\\] Po pomiarze kubitu, czyli na końcu procesu obliczeniowego, ze względu na prawa fizyki otrzymujemy tylko i wyłącznie jeden ze stanów bazowych \\(\\ket{0}\\) lub \\(\\ket{1}\\). Każdy następny pomiar (tej samej obserwabli) będzie kończyc się w tym samym (otrzymanym) stanie.\n\nPomiar niszczy superpozycję kubitu i sprowadza go do jednego ze stanów bazowych.\n\nDla kubitu w superpozycji stanów bazowych jedyne co możemy określic to prawdopodobieństwo otrzymania stanu \\(\\ket{0}\\) i \\(\\ket{1}\\).\n\nPrawdopodobieństwo określone jest jako kwadrat (modułu) amplitudy Dla stanu \\(\\ket{0}\\) \\(P(0) = |\\alpha|^2\\) oraz dla stanu \\(\\ket{1}\\) \\(P(1)= |\\beta|^2\\).\n\nIstnieje możliwośc pomiaru kubitów w innych bazach. Jednak w większości przypadków ograniczymy się do pomiaru w bazie obliczeniowej.\n\n\n\nPrzykład\nRozważmy stan \\[\\ket{\\psi} = \\frac{\\sqrt{3}}{2}\\ket{0}+\\frac{1}{2}\\ket{1}\\]\nMożliwe wyniki pomiaru w bazie Z \\(\\{ \\ket{0},\\ket{1} \\}\\).\n\\[\n\\braket{0}{\\psi} = \\bra{0}\\left( \\frac{\\sqrt{3}}{2}\\ket{0} +\\frac{1}{2}\\ket{1}\\right) = \\frac{\\sqrt{3}}{2}\\braket{0}{0} + \\frac{1}{2}\\braket{0}{1} = \\frac{\\sqrt{3}}{2}\n\\] Biorąc kwadrat apmlitudy otrzymujemy kubit w stanie \\(\\ket{0}\\) z prawdopodobieństwem \\(0.75\\). \\[\n\\braket{1}{\\psi} = \\bra{1}\\left( \\frac{\\sqrt{3}}{2}\\ket{0} +\\frac{1}{2}\\ket{1}\\right) = \\frac{\\sqrt{3}}{2}\\braket{1}{0} + \\frac{1}{2}\\braket{1}{1} = \\frac{1}{2}\n\\] Biorąc kwadrat apmlitudy otrzymujemy stan \\(\\ket{1}\\) z prawdopodobieństwem \\(0.25\\).\n\\[\\ket{\\psi} = \\braket{0}{\\psi}\\ket{0} + \\braket{1}{\\psi}\\ket{1}\\]\nDowolna para liniowo niezależnych wektorów jednostkowych \\(\\ket{u}\\) i \\(\\ket{v}\\) pochodząca z dwuwymiarowej przestrzeni wektorowej może tworzyc bazę: \\[\n\\alpha \\ket{0} +\\beta \\ket{1} = \\alpha' \\ket{u} +\\beta' \\ket{v}\n\\] Przykładem może byc tzw Baza Hadamarda \\(\\ket{+}\\) i \\(\\ket{-}\\) zdefiniowana jako: \\[\n\\ket{+} = \\frac{1}{\\sqrt{2}}(\\ket{0}+\\ket{1}) = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n\\] \\[\n\\ket{-} = \\frac{1}{\\sqrt{2}}(\\ket{0}-\\ket{1}) = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ - \\frac{1}{\\sqrt{2}} \\end{bmatrix}\n\\]\n\nBardzo ważnym etapem jest wybór bazy w której dokonujemy pomiaru. np. dla wektora \\(\\ket{+}\\) pomiar w bazie standardowej pozwoli otrzymac wyniki stanu \\(\\ket{0}\\) i \\(\\ket{1}\\) z prawdopodobieństwami \\(\\frac{1}{2}\\). Natomiast jeśli pomiar dokonywany byłby w bazie Hadamarda to zawsze otrzymamy stan \\(\\ket{+}\\) z prawdopodobieństwem 1.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Przestrzenie wektorowe, stany kwantowe, reprezentacja klasycznych i kwantowych bitów"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Ciekawe książki i strony internetowe zamieszczone zostały w zakładce książki. Jeśli chciał(a)byś coś dodać prześlij informację przez MS teams.",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Ciekawe książki i strony internetowe zamieszczone zostały w zakładce książki. Jeśli chciał(a)byś coś dodać prześlij informację przez MS teams.",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład\n\n\n21.12.2025\n\n11.01.2025\n24.01.2025\n25.01.2025\n01.02.2026",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nAlgebra liniowa - wektory, macierze, baza, iloczyn skalarny, iloczyn tensorowy\nPython, Jupyter notebook, Jupyter lab, Colab\nAlgorytmy sieci neuronowych i uczenia maszynowego w procesie klasyfikacji binarnej",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#qpoland",
    "href": "index.html#qpoland",
    "title": "Informacje ogólne",
    "section": "QPoland",
    "text": "QPoland\nQPoland jest cześcią międzynarodowej sieci QWorld.\nZapraszamy do wspólnego poszerzania wiedzy. Szczegóły &gt; QWorld is a global network of individuals, groups, and communities collaborating on education and implementation of quantum technologies and research activities.",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "Chris Bernhardt, Obliczenia kwantowe dla każdego. PWN 2020\n\n\nWyjaśnienie jak to działa w obliczeniach kwantowych.\n\n\nMichel Le Bellac, Wstęp do informatyki kwantowej. PWN 2011\n\n\nTrudniejsza, dużo matematyki i fizyki.\n\n\nThomas G. Wong, Introduction to Classical and Quantum Computing. Rooted Grove 2022.\n\n\nBardzo dobra!, dużo przykładów, dużo ciekawych informacji wyjaśnianych bardzo szczegółowo.\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nP. Gawron, M. Cholewa, … Rewolucja stanu. Fantastyczne wprowadzenie do informatyki kwantowej. Quantumz.io 2021\nA. Saxena, J. Mancilla, I. Montalban, C. Pere, Financial Modeling Using Quantum Computing. Packt 2023\nM. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Springer 2021.\n\n\n\n\n\n\n\nL. Moroney, Sztuczna inteligencja i uczenie maszynowe dla programistów. Praktyczny przewodnik po sztucznej inteligencji. Helion 2021. Zobacz opis lub Kup e-book\nBruce, Bruce, Gedeck, Statystyka praktyczna w data science. Wydanie II. Helion. 2021."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "Chris Bernhardt, Obliczenia kwantowe dla każdego. PWN 2020\n\n\nWyjaśnienie jak to działa w obliczeniach kwantowych.\n\n\nMichel Le Bellac, Wstęp do informatyki kwantowej. PWN 2011\n\n\nTrudniejsza, dużo matematyki i fizyki.\n\n\nThomas G. Wong, Introduction to Classical and Quantum Computing. Rooted Grove 2022.\n\n\nBardzo dobra!, dużo przykładów, dużo ciekawych informacji wyjaśnianych bardzo szczegółowo.\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nP. Gawron, M. Cholewa, … Rewolucja stanu. Fantastyczne wprowadzenie do informatyki kwantowej. Quantumz.io 2021\nA. Saxena, J. Mancilla, I. Montalban, C. Pere, Financial Modeling Using Quantum Computing. Packt 2023\nM. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Springer 2021.\n\n\n\n\n\n\n\nL. Moroney, Sztuczna inteligencja i uczenie maszynowe dla programistów. Praktyczny przewodnik po sztucznej inteligencji. Helion 2021. Zobacz opis lub Kup e-book\nBruce, Bruce, Gedeck, Statystyka praktyczna w data science. Wydanie II. Helion. 2021."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nPeter Shor Wykład\n\n\nPakiety Python\n\nQiskit\nPennyLane\n\n\n\nPakiety Julia\n\nYao\nQAOA\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nFilmy\n\nWprowadzenie do obliczeń kwantowych\nQPoland, Bronze, Warsztaty z programowania komputerów kwantowych 2023"
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3 -m venv &lt;name of env&gt;\n\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n\n(venv)$ \nJak uruchomić środowisko pythona w systemie Windows.\nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3 -m venv &lt;name of env&gt;\n\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n\n(venv)$ \nJak uruchomić środowisko pythona w systemie Windows.\nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne (np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "lectures/kodyw1.html",
    "href": "lectures/kodyw1.html",
    "title": "Modele uczenia maszynowego",
    "section": "",
    "text": "Do wygenerowania kodów użyjemy biblioteki PyTorch",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/kodyw1.html#regresja-liniowa",
    "href": "lectures/kodyw1.html#regresja-liniowa",
    "title": "Modele uczenia maszynowego",
    "section": "Regresja liniowa",
    "text": "Regresja liniowa\nWygenerujemy niezaszumione dane na podstawie wzoru \\(y = 2 x - 1\\). Na podstawie zbioru danych postaramy się oszacować nieznane parametry czyli wyraz przy \\(x\\) (\\(\\alpha_1 = 2\\)) i wyraz wolny (\\(\\alpha_0 = -1\\)).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# zbior danych\nx = range(11)\ny = [2*xi - 1 for xi in x]\nplt.plot(x, y, 'go', label='True data', alpha=0.5)\n\n\n\n\n\n\n\n\nModel regresji liniowej dla jednej zmiennej można zrealizować jako prostą jednowarstwową sieć neuronową. Cały proces można zrealizować za pomocą obiektu torch.nn.Linear\n\nimport torch\n\nclass LinearRegression(torch.nn.Module):\n\n    def __init__(self, inputSize, outputSize):\n        super(LinearRegression, self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(inputSize, outputSize)\n        ) \n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nm = torch.nn.Linear(1,1)\n\n\nm\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nm.weight, m.bias\n\n(Parameter containing:\n tensor([[-0.1154]], requires_grad=True),\n Parameter containing:\n tensor([0.4927], requires_grad=True))\n\n\nAby nasze dane mogłybyć przeliczane przez bibliotekę PyTorch musimy je przetworzyć na tensory - czyli obiekty z biblioteki PyTorch.\n\n# dostosowanie do pytorch\nx = np.array(x, dtype=np.float32)\ny = np.array(y, dtype=np.float32)\n\nX_train = torch.from_numpy(x).view(-1,1)\ny_train = torch.from_numpy(y).view(-1,1)\n\nUwaga - ponieważ mamy jedną zmienną zawierającą 10 przypadków - potrzebujemy listy składającej się z 10 list jednoelementowych.\nMozna tez wykorzystac obiektowe programowanie.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass LinearDataset(Dataset):\n    def __init__(self, X_train, y_train):\n        self.X_train = X_train # tensor typu torch\n        self.y_train = y_train\n\n    def __len__(self):\n        return len(self.y_train)\n\n    def __getitem__(self, idx):\n        return self.X_train[idx], self.y_train[idx]\n\n\ndataset = LinearDataset(X_train=X_train, y_train=y_train)\n\n\ndataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n\nMożemy utworzyć model i wybrać optymalizator z funkcją kosztu.\n\n# obiekt liniowej regresji w wersji sieci nn\nlr_model = LinearRegression(1,1)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)\n\nMożemy sprawdzić, że nasz model będzie dostrajał 2 parametry.\n\nnum_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)\nprint(f\"liczba trenowalnych parametrów: {num_params}\")\n\nliczba trenowalnych parametrów: 2\n\n\nParametry te w początkowej inicjalizacji mają następujące wartości:\n\nfor layer in lr_model.layers:\n    if isinstance(layer, torch.nn.Linear):\n        print(f\"weight: {layer.state_dict()['weight']}\")\n        print(f\"bias: {layer.state_dict()['bias']}\")\n\nweight: tensor([[0.7394]])\nbias: tensor([0.1832])\n\n\n\nepochs = 400\n# petla uczaca \nfor epoch in range(epochs):\n    lr_model.train() # etap trenowania \n\n    y_pred = lr_model(X_train)\n    loss = criterion(y_pred, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.4f}')\n \n    lr_model.eval() # etap ewaluacji modelu\n\n# po treningu jeszcze raz generujemy predykcje\nlr_model.eval()\nwith torch.no_grad():\n    predicted = lr_model(X_train)\n\nepoch: 050, loss = 0.2946\nepoch: 100, loss = 0.1681\nepoch: 150, loss = 0.0959\nepoch: 200, loss = 0.0547\nepoch: 250, loss = 0.0312\nepoch: 300, loss = 0.0178\nepoch: 350, loss = 0.0101\nepoch: 400, loss = 0.0058\n\n\nMozna tez wykorzystac obiekt dataloader\n\nfor epoch in range(50):\n    for X_batch, y_batch in dataloader:\n        preds = lr_model(X_batch)\n        loss = criterion(preds, y_batch)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, loss = {loss.item():.4f}\")\n\nEpoch 0, loss = 0.0060\nEpoch 10, loss = 0.0046\nEpoch 20, loss = 0.0000\nEpoch 30, loss = 0.0014\nEpoch 40, loss = 0.0001\n\n\nOtrzymane parametry po uczeniu\n\nprint(f\"po procesie uczenia waga: {lr_model.layers[0].weight} oraz bias {lr_model.layers[0].bias}\")\n\npo procesie uczenia waga: Parameter containing:\ntensor([[1.9952]], requires_grad=True) oraz bias Parameter containing:\ntensor([-0.9755], requires_grad=True)\n\n\nDopasowanie modelu do danych można przedstawić na wykresie\n\nplt.clf()\nplt.plot(X_train, y_train, 'go', label='True data', alpha=0.5)\nplt.plot(X_train, predicted, '--', label='Predictions', alpha=0.5)\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport torch\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\ntorch.manual_seed(1234)\n\n# DANE \nx = torch.linspace(0,10,500).view(-1,1)\ny = torch.sin(x)\ny = y + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x,y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nclass SinusEstimator(torch.nn.Module):\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int):\n        super(SinusEstimator,self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(N_INPUT, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,16),\n            torch.nn.Tanh(),\n            torch.nn.Linear(16,N_OUTPUT)\n        )\n\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nmodel = SinusEstimator(1,1)\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\nlosses = []\n\n\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\n\n\ndef train(X,Y, model, optimiser, epochs, lossfn, callback = None):\n    for epoch in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\n\n\nx_train = x.requires_grad_(False)\n\ntrain(x_train, y, model, optimiser, 300, criterion, callback)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    return torch.mean((y-y_pred)**2)\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n\nmodel2 = SinusEstimator(1,1)\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model2.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\nlosses = []\n\ntrain(x_train, y, model2, optimiser, 200, special_loss_fn, callback)",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/kodyw1.html#regresja-logistyczna",
    "href": "lectures/kodyw1.html#regresja-logistyczna",
    "title": "Modele uczenia maszynowego",
    "section": "Regresja logistyczna",
    "text": "Regresja logistyczna\nW przypadku procesu klasyfikacji danych do numerycznego wyniku musimy dodać funkcję aktywacji - sigmoid \\(\\sigma\\), która pozwoli nam wygenerować prawdopodobieństwo otrzymania klasy 1.\nDane wygenerujemy na podstawie pakietu scikit-learn\n\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# prepare dataset\nX, y = make_classification(n_samples=10**4, n_features=10 ,random_state=42)\n\n\nimport torch\n\nclass LogisticRegression(torch.nn.Module):\n\n    def __init__(self, inputSize, outputSize):\n        super(LogisticRegression, self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(inputSize, outputSize),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\nPodobnie jak w przypadku regresji liniowej musimy przetworzyć nasze dane do obiektów torch.\n\nX_train = torch.from_numpy(X.astype(np.float32))\ny_train = torch.from_numpy(y.astype(np.float32))\ny_train = y_train.view(y_train.shape[0], 1)\n\n\nmodel = LogisticRegression(X_train.shape[1], y_train.shape[1])\n\nlearningRate = 0.01\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n\n# petla uczaca \nnum_epochs = 500\n\nfor epoch in range(num_epochs):\n    # forward pass and loss\n    model.train()\n    y_predicted = model(X_train)\n    loss = criterion(y_predicted, y_train)\n    \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    model.eval()\n\n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    y_predicted = model(X_train)  # no need to call model.forward()\n    y_predicted_cls = y_predicted.round()   # round off to nearest class\n    acc = y_predicted_cls.eq(y_train).sum() / float(y_train.shape[0])  # accuracy\n    print(f'accuracy = {acc:.4f}')\n    print(f\"predykcja dla wiersza 0:{y_predicted[0]}, wartosc prawdziwa: {y_train[0]}\")\n\nepoch: 50, loss = 0.6307\nepoch: 100, loss = 0.5104\nepoch: 150, loss = 0.4487\nepoch: 200, loss = 0.4120\nepoch: 250, loss = 0.3878\nepoch: 300, loss = 0.3708\nepoch: 350, loss = 0.3581\nepoch: 400, loss = 0.3484\nepoch: 450, loss = 0.3406\nepoch: 500, loss = 0.3343\naccuracy = 0.8830\npredykcja dla wiersza 0:tensor([0.8189]), wartosc prawdziwa: tensor([1.])",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html",
    "href": "lectures/cwiczenia1.html",
    "title": "Pennylane wprowadzenie",
    "section": "",
    "text": "import torch\nimport matplotlib.pyplot as plt \nfrom IPython.display import clear_output\n\n\nx = torch.linspace(0,10,500).view(-1,1)\n\nsi = torch.sin(x).view(-1,1)\n\ny = si + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x, y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\nclass QN(torch.nn.Module):\n    '''Classical -&gt; Quantum -&gt; Classical'''\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int, Q_NODE, N_QUBITS):\n        super().__init__()\n\n        self.layers = torch.nn.Sequential(\n            # input layer\n            torch.nn.Linear(N_INPUT, N_QUBITS),\n            # 1st hidden layer as a quantum circuit\n            Q_NODE,\n            # output layer\n            torch.nn.Linear(N_QUBITS, N_OUTPUT)\n        )\n        \n\n    def forward(self, x):\n        return  self.layers(x)\nimport pennylane as qml\n\nn_qubits = 3\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n\nn_layers = 3\n\nweight_shapes = {\"weights\": (n_layers, n_qubits)}\nqlayer = qml.qnn.TorchLayer(qnode, weight_shapes)\nqmodel = QN(1, 1, qlayer, n_qubits)\n\nlearning_rate=1e-3\n\noptimiser = torch.optim.Adam(qmodel.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\nlosses = []\nx[:5], y[:5]\n\n(tensor([[0.0000],\n         [0.0200],\n         [0.0401],\n         [0.0601],\n         [0.0802]]),\n tensor([[ 0.0067],\n         [-0.0128],\n         [ 0.0702],\n         [ 0.0238],\n         [ 0.1135]]))\nqmodel.train()\nprediction = qmodel(x)\nprediction[:5]\n\ntensor([[-0.3303],\n        [-0.3310],\n        [-0.3316],\n        [-0.3323],\n        [-0.3329]], grad_fn=&lt;SliceBackward0&gt;)\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\ndef train(X, Y, model, optimiser, epochs, lossfn, callback = None):\n    for _ in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\ntrain(x, y, qmodel, optimiser,500, criterion, callback)",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#pennylane",
    "href": "lectures/cwiczenia1.html#pennylane",
    "title": "Pennylane wprowadzenie",
    "section": "🧠 PennyLane",
    "text": "🧠 PennyLane\n🔗 Strona oficjalna\nPennyLane to otwarto‑źródłowa biblioteka Pythona opracowana przez Xanadu dla kwantowego uczenia maszynowego, obliczeń kwantowych oraz chemii kwantowej.\nZapewnia wysokopoziomowy, intuicyjny interfejs do budowania hybrydowych modeli kwantowo‑klasycznych, łącząc obwody kwantowe z popularnymi frameworkami uczenia maszynowego, takimi jak PyTorch i TensorFlow.\nPennyLane wprowadza pojęcie QNode (quantum node) – funkcji kwantowych, które zachowują się jak zwykłe funkcje Pythona i obsługują automatyczną różniczkowanie (autodiff).\nUmożliwia uruchamianie modeli zarówno na symulatorach, jak i na rzeczywistym sprzęcie kwantowym (np. IBM Q, Amazon Braket i inne).\nDzięki PennyLane możesz: - budować wariacyjne algorytmy kwantowe,\n- trenować kwantowe sieci neuronowe,\n- eksplorować zaawansowane architektury kwantowego uczenia maszynowego.\nBiblioteka stanowi potężny most między klasyczną sztuczną inteligencją a rosnącym światem obliczeń kwantowych.\nPennyLane zawiera także spersonalizowaną wersję NumPy (pennylane.numpy), która obsługuje tablice śledzone gradientem, co ułatwia integrowanie obwodów kwantowych w procesach optymalizacji.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#podstawowe-importy-bibliotek",
    "href": "lectures/cwiczenia1.html#podstawowe-importy-bibliotek",
    "title": "Pennylane wprowadzenie",
    "section": "podstawowe importy bibliotek",
    "text": "podstawowe importy bibliotek\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\n\n🧪 Obwody kwantowe w PennyLane\n\nObwody kwantowe są implementowane jako funkcje kwantowe, zwane także QNode’ami.\nSą to funkcje kwantowe zachowujące się jak standardowe funkcje Pythona i wspierające automatyczną różniczkację przy użyciu klasycznych narzędzi ML.\nQNode’y są uruchamiane na różnych urządzeniach (devices), takich jak:\n\nsymulatory (np. default.qubit, lightning.qubit) oraz,\nrzeczywisty sprzęt kwantowy (np. IBM Q, Amazon Braket, Xanadu).\n\nUrządzenia są wymienne i określają, w jaki sposób dana funkcja kwantowa jest wykonywana.\n\n\n\n\nPennyLane\n\n\n🔗 Urządzenia, które możesz używać\nMożemy zdefiniować nasz symulator — w tym przypadku użyjemy default.qubit.\nMusimy także określić, ile kubitów chcemy użyć, korzystając z parametru wires.\nPrzykładowe urządzenia\n\ndefault.qubit – symulator napisany w Pythonie\n\nlightning.qubit – szybszy symulator napisany w C++\n\ndefault.mixed – używany do symulacji mieszanych stanów kwantowych\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n\n## for example \ndev2 = qml.device(\"default.qubit\", wires=3)\n\ndev3 = qml.device(\"lightning.qubit\", wires=['q1', 'aux'])\n\n\n\n\nkubity1\n\n\n\\[\n\\ket{000} = \\ket{0}\\otimes \\ket{0} \\otimes \\ket{0}\n\\]\n\nObiekt Qnode będziemy używać do definicji obwodów kwantowych. Obiekt ten wspiera wiele bibliotek do obliczeń numerycznych, tzw. interfejsów: - NumPy, - PyTorch, - TensorFlow, - JAX\nDomyślnie QNodes używa interfejs NumPy. Dzięki niemu mamy dostęp do optymalizatorów domyślnych z biblioteki Pennylane. Pozostałe interferjsy wymagają użycia optymalizatorów z innych pakietów.\n\ndef qc(): # quantum circuit\n    return qml.state()\n\nwires oznacza kwantowy podsystem - czyli nasz pojedynczy kubit. Liczymy od 0 nie od 1.\n\nFunkcja kwantowa może pobierać klasyczne pamaretry\nFunkcja kwantowa może zawierać klasyczny flow (przepływ) twojego programu for czy if else.\n\nZbiór kwantowych operatorów",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#uruchomienie-obwodu-kwantowego",
    "href": "lectures/cwiczenia1.html#uruchomienie-obwodu-kwantowego",
    "title": "Pennylane wprowadzenie",
    "section": "Uruchomienie obwodu kwantowego",
    "text": "Uruchomienie obwodu kwantowego\nUruchomienie odbywa się po wyborze device z określeniem ilości kubitów (wires)\n\ncirc = qml.QNode(qc, dev)\ncirc()\n\narray([1.+0.j, 0.+0.j])\n\n\n\\[\n\\ket{\\psi} = \\ket{0} = [1,0]^{T}\n\\]\n\ncirc2 = qml.QNode(qc, dev2)\ncirc2()\n\narray([1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j, 0.+0.j])\n\n\n\ncirc3 = qml.QNode(qc, dev3)\ncirc3()\n\narray([1.+0.j, 0.+0.j, 0.+0.j, 0.+0.j])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\ndef quantum_circuit():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\ncirc = qml.QNode(quantum_circuit, dev)\n\ncirc()\n\narray([0.70710678+0.j, 0.70710678+0.j])\n\n\n\nfrom math import sqrt\nprint(circ()[0].real, 1/sqrt(2))\nprint(circ()[0].real == 1/sqrt(2))\n\n0.7071067811865475 0.7071067811865475\nTrue\n\n\n\nqml.draw(circ)()\n\n\nqml.draw_mpl(circ)()\n\nInna wersja\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.70710678+0.j, 0.70710678+0.j])\n\n\n\nimport matplotlib.pyplot as plt\n\nqml.drawer.use_style(\"pennylane_sketch\")\n\nfig, ax = qml.draw_mpl(qc)()\nplt.show()\n\nMatplotlib is building the font cache; this may take a moment.\n\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.probs()\n\nqc()\n\narray([0.5, 0.5])\n\n\nDLa pustego obwodu\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    return qml.probs()\n\nresults = qc()\nresults\n\narray([1., 0.])\n\n\n\nużyj qml.sample() lub qml.counts() dla innych wariantów wyników.\n\nIlość wykonań obwodu sterowana jest w QNode za pomocą parametru shot, który może być liczbą jak również listą liczb. &gt; Uwaga w wersji biblioteki &lt;0.43 - parametr shot ustawiany jest na poziomie device.\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev, shots=5)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.sample()\n\nqc()\n\narray([[1],\n       [0],\n       [0],\n       [0],\n       [1]])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev, shots=100)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.counts()\n\nresults = qc()\n\n\nresults\n\n{np.str_('0'): np.int64(54), np.str_('1'): np.int64(46)}\n\n\nKod naszej wartwy ukrytej w której użyliśmy obwodu kwantowego realizował następujące obiekty i funkcje:\n\nimport pennylane as qml\n\nn_qubits = 2\ndev = qml.device(\"default.qubit\", wires=n_qubits)\n\n@qml.qnode(dev)\ndef qnode(inputs, weights):\n    qml.AngleEmbedding(inputs, wires=range(n_qubits))\n    qml.BasicEntanglerLayers(weights, wires=range(n_qubits))\n    return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\nObwody kwantowe składają się z rejestrów, które reprezentują poszczególne kubity.\n\nDomyślnie kubity inicjalizujemy w stanie 0.\n\nOperacje wykonywane na kubitach nazywamy bramkami.\nOperacje te można wykonywać na jednym albo i wielu kubitach na raz.\nDomyślnie będziemy optymalizować algortymy aby składały się z jak najmniejszej ilości bramek działających na dużą liczbę kubitów.\nGraficznie można rozumieć realizację algorytmu jako stosowanie bramek na poszczególnych kubitach.\n\n\n\nkibu2\n\n\nW bibliotece PennyLane, obwody kwantowe reprezentowane są przez kwantowe funkcje, realizowane przez klasyczne funkcje w pythonie.\nSchemat kodu penny lane możemy zapisać jako:\nimport pennylane as qml\n\ndef my_quantum_function(params):\n\n    # Single-qubit operations with no input parameters\n    qml.Gate1(wires=0)\n    qml.Gate2(wires=2)\n\n     # Two-qubit operation with no input parameter on wires 0 and 1\n    qml.TwoQubitGate1(wires=[0, 1])\n\n    # A single-qubit operation with an input parameter\n    qml.Gate3(params[0], wires=2)\n\n\n    # Two-qubit operation with an input parameter on wires 0 and 1\n    qml.TwoQubitGate2(params[1], wires=[1, 2])\n    ... \n\n    # Return the result of a measurement\n    return qml.Measurement(wires=[0, 1])\nMatematycznie całość możemy zapisać jako:\n\nPrzykładowo\n\n\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=2)\n#dev = qml.device(\"default.qubit\", wires=2, shots=1000)\n\n@qml.qnode(dev)\ndef circ(theta):\n    qml.Hadamard(wires = 0)\n    qml.CNOT(wires = [0,1])\n    qml.RZ(theta, wires = 0)\n    return qml.state()\n#    return qml.probs(wires = [0,1])\n\ncirc(np.pi)\n\narray([4.32978028e-17-0.70710678j, 0.00000000e+00+0.j        ,\n       0.00000000e+00+0.j        , 4.32978028e-17+0.70710678j])\n\n\n\nprint(qml.draw(circ)(np.pi))\n\n0: ──H─╭●──RZ(3.14)─┤  State\n1: ────╰X───────────┤  State\n\n\n\nqml.draw_mpl(circ, decimals=2, style=\"sketch\", level=\"device\")(np.pi)\n\n\n\n\n\n\n\n\n\nBramka X\nBramka X-gate reprezentowana jest przez macierz Pauli-X :\n\\[\nX = \\begin{pmatrix}\n0 & 1 \\\\\n1 & 0 \\\\\n\\end{pmatrix}\n\\]\nBramka X obraca kubit w kierunku osi na sferze Bloch’a o \\(\\pi\\) radianów. Zmienia \\(|0\\rangle\\) na \\(|1\\rangle\\) oraz \\(|1\\rangle\\) na \\(|0\\rangle\\). Jest często nazywana kwantowym odpowiednikiem bramki NOT lub określana jako bit-flip.\n\\[ \\sigma_x \\ket{0} = \\ket{1} \\,\\,\\, \\sigma_x\\ket{1} = \\ket{0} \\]\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.X(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.+0.j, 1.+0.j])\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.PauliX(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.+0.j, 1.+0.j])\n\n\n\nqml.draw_mpl(qc)()\n\n\n\n\n\n\n\n\n\n\nBramka Hadamarda\nBramka Hadamarda przetwarza stan \\(|0\\rangle\\) na kombinacje liniowa (superpozycje) \\(\\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}}\\), co oznacza, że pomiar zwróci z takim samym prawdopodobieństwem stanu 1 lub 0. Stan ten często oznaczany jest jako: \\(|+\\rangle\\).\n\\[\nH = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}\n1 & 1 \\\\\n1 & -1 \\\\\n\\end{pmatrix}\n\\]\n\\[ H\\ket{0} = \\frac{\\sqrt{2}}{2} (\\ket{0}+ \\ket{1})\\] \\[ H\\ket{1} = \\frac{\\sqrt{2}}{2}(\\ket{0}- \\ket{1})\\]\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\nqc()\n\narray([0.70710678+0.j, 0.70710678+0.j])\n\n\nobwód z elementami pythona\n\ndev = qml.device(\"default.qubit\", wires=1)\n\n@qml.qnode(dev)\ndef qc(state):\n    if state==1:\n        qml.X(wires=0)\n    qml.Hadamard(wires=0)\n    qml.PauliX(wires=0)\n    qml.Hadamard(wires=0)\n    return qml.state()\n\nqc(0)\n\n\n\nbramka SX\nBramka SX jest pierwiastkiem kwadratowym bramki X. Dwukrotne zastosowanie powinno reazlizowac bramkę X.\n\\[\nSX = \\frac{1}{2}\\begin{pmatrix}\n1+i & 1-i \\\\\n1-i & 1+i \\\\\n\\end{pmatrix}\n\\]\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.SX(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([0.5+0.5j, 0.5-0.5j])\n\n\n\n\n\n\n\n\n\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.SX(wires=0)\n    qml.SX(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([0.+0.j, 1.+0.j])\n\n\n\n\n\n\n\n\n\n\n\nZ gate\n\\[\nZ = \\begin{pmatrix}\n1 & 0 \\\\\n0 & -1 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & e^{i \\pi} \\\\\n\\end{pmatrix}\n\\]\nInne nazwy bramki: phase flip lub sign flip\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\ndef qc():\n    qml.Z(wires=0)\n    return qml.state()\n\nqml.draw_mpl(qc)()\nqc()\n\narray([ 1.+0.j, -0.+0.j])\n\n\n\n\n\n\n\n\n\n\n\nRZ gate\nBramkę PauliZ można uogólnić i sparametryzować kątem. Dla \\(\\phi=\\pi\\) otrzymujemy bramkę \\(\\sigma_z\\).\n\\[\n\\begin{pmatrix}\n1 & 0 \\\\\n0 & -1 \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & e^{i \\pi} \\\\\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 \\\\\n0 & e^{i \\phi} \\\\\n\\end{pmatrix}\n\\]\n\\[ R_Z(\\phi) = e^{-i \\phi \\frac{\\sigma_z}{2} }  \\]\n\\[\nRZ = \\begin{pmatrix}\ne ^{-i \\frac{\\phi}{2} } & 0 \\\\\n0 & e ^{i \\frac{\\phi}{2} } \\\\\n\\end{pmatrix} = \\cos(\\frac{\\phi}{2})I_2 - \\sin(\\frac{\\phi}{2}) i\\sigma_z\n\\]\n\nfrom pennylane import numpy as np\n\ndev = qml.device(\"default.qubit\", wires=1)\n@qml.qnode(dev)\n\n\ndef qc(phi):\n    qml.RZ(phi=phi, wires=0)\n    return qml.state()\n\nqc(np.pi/2)\n\narray([0.70710678-0.70710678j, 0.        +0.j        ])\n\n\n\nqml.draw_mpl(qc)(np.pi/2)\n\n\n\n\n\n\n\n\n\n\nCNOT\nJedną z bramek realizującą zadania na dwóch kubitach jest bramka CNOT, która na bazie bitu kontrolnego decyduje czy zastosować operację X do drugiego kubitu.\n\\[\n\\text{CNOT} = \\begin{bmatrix} 1 \\,\\, \\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\\\\n0\\,\\, \\,\\,\\, 1 \\,\\,\\,\\,\\, 0 \\,\\,\\,\\,\\, 0 \\\\\n0\\,\\,\\,\\,\\, 0\\,\\,\\,\\,\\,  0 \\,\\,\\,\\,\\, 1 \\\\ 0\\,\\,\\,\\,\\, 0\\,\\,\\,\\,\\, 1\\,\\,\\,\\,\\, 0 \\end{bmatrix}\n\\]\n\\[ \\text{CNOT} \\ket{00} = \\ket{00} \\]\n\\[ \\text{CNOT} \\ket{10} = \\ket{11} \\]\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef circ(stan='0'):\n    if stan == '1':\n        qml.X(wires=0)\n    qml.CNOT(wires=[0,1])\n    # qml.CNOT(wires=[1,0])\n    return qml.state()\n\n\nstate = circ()\nprint(state)\n\n[1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n\n\n\nstate = circ('1')\nprint(state)\n\n[0.+0.j 0.+0.j 0.+0.j 1.+0.j]\n\n\nMagic ;)\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=2, shots=100)\n\n@qml.qnode(dev)\ndef qc():\n    # qml.Hadamard(wires=0)\n    qml.X(wires=0)\n    qml.CNOT(wires=[0,1])\n    #return qml.state()\n    return qml.counts()\n\nqc()\n\n/Users/seba/Documents/GitHub/technologiekwantowe/.venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\n{np.str_('11'): np.int64(100)}\n\n\n\nimport matplotlib.pyplot as plt\nqml.drawer.use_style(\"sketch\")\nfig, ax = qml.draw_mpl(qc)()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=2, shots=100)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    qml.CNOT(wires=[0,1])\n    qml.X(wires=1)\n    #return qml.state()\n    return qml.counts()\n\nqc()\n\n/Users/seba/Documents/GitHub/technologiekwantowe/.venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\n{np.str_('01'): np.int64(51), np.str_('10'): np.int64(49)}\n\n\n\nfig, ax = qml.draw_mpl(qc)()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=2, shots=100)\n\n@qml.qnode(dev)\ndef qc():\n    qml.Hadamard(wires=0)\n    qml.CNOT(wires=[0,1])\n    qml.X(wires=1)\n    qml.Z(wires=1)\n    #return qml.state()\n    return qml.counts()\n\nqc()\n\n/Users/seba/Documents/GitHub/technologiekwantowe/.venv/lib/python3.13/site-packages/pennylane/devices/device_api.py:193: PennyLaneDeprecationWarning: Setting shots on device is deprecated. Please use the `set_shots` transform on the respective QNode instead.\n  warnings.warn(\n\n\n{np.str_('01'): np.int64(43), np.str_('10'): np.int64(57)}",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#zadanie---obwód-kwantowy-z-optymalizacją",
    "href": "lectures/cwiczenia1.html#zadanie---obwód-kwantowy-z-optymalizacją",
    "title": "Pennylane wprowadzenie",
    "section": "Zadanie - Obwód kwantowy z optymalizacją",
    "text": "Zadanie - Obwód kwantowy z optymalizacją\n\nNapisz nowy obwód kwantowy, który zawierać będzie tylko bramkę \\(R_X\\) dla dowolnego parametru \\(\\theta\\)\noblicz i uzasadnij, że wartość oczekiwana dla stanu \\(\\ket{\\psi} = R_X \\, \\ket{0}\\) \\[&lt;Z&gt; = cos^2(\\theta /2)- sin^2(\\theta /2) = cos(\\theta)\\]\n\nZałóżmy, że nasz problem obliczeniowy sprowadza się do wygenerowania wartości oczekiwanej o wartości 0.5.\n\\[\n\\textbf{&lt;Z&gt;} = \\bra{\\psi} \\textbf{Z} \\ket{\\psi} = 0.5\n\\]\nNapisz program znajdujący rozwiązanie - szukający wagę \\(\\theta\\) dla naszego obwodu\n\nZdefiniuj funkcję kosztu, którą bedziemy minimalizować \\((Y - y)^2\\)\nzainicjuj rozwiązanie \\(theta=0.01\\) i przypisz do tablicy array np.array(0.01, requires_grad=True)\nJako opt wybierz spadek po gradiencie : opt = qml.GradientDescentOptimizer(stepsize=0.1)\nuzyj poniższego kodu do wygenerowania pętli obiczeń\n\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.5)**2\n\ntheta = np.array(0.01, requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: 0.010999883335916642, koszt: 0.24993950555333252\nepoka: 10, theta: 0.028520883980330904, koszt: 0.2495934725570593\nepoka: 20, theta: 0.07380240366299132, koszt: 0.24728524869432472\nepoka: 30, theta: 0.18848123038996684, koszt: 0.23260358196368314\nepoka: 40, theta: 0.44553231822816797, koszt: 0.1619107886095973\nepoka: 50, theta: 0.7954652635692223, koszt: 0.03998102446252434\nepoka: 60, theta: 0.9838691671205075, koszt: 0.002894983645374295\nepoka: 70, theta: 1.0340365114010706, koszt: 0.00012891702079013002\nepoka: 80, theta: 1.0445781695789977, koszt: 5.138079127884816e-06\nepoka: 90, theta: 1.0466807535250837, koszt: 2.002500944777545e-07\nOptymalizacja zakonczona dla theta=1.0470778036429096, koszt: 1.0753863888581739e-08\n\n\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev, interface=\"torch\")\ndef par_c(theta):\n    qml.RX(theta, wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\n\ndef cost_fn(theta):\n    target = 0.5\n    return (par_c(theta) - target) ** 2\n\n\nimport torch\nfrom torch.optim import Adam \n\ntheta = torch.tensor(0.01, requires_grad=True)\n\noptimizer = Adam([theta], lr=0.1)\nepochs = 100\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    loss = cost_fn(theta)\n    loss.backward()\n    optimizer.step()\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: 0.1099998876452446, koszt: 0.24399264948886004\nepoka: 10, theta: 1.0454959869384766, koszt: 2.169397961785511e-06\nepoka: 20, theta: 1.0966185331344604, koszt: 0.0018829460500888265\nepoka: 30, theta: 0.9526112079620361, koszt: 0.006329338284936267\nepoka: 40, theta: 1.111649513244629, koszt: 0.0032281231974498922\nepoka: 50, theta: 1.0076401233673096, koszt: 0.0011463367423114523\nepoka: 60, theta: 1.0690317153930664, koszt: 0.00036201344599675586\nepoka: 70, theta: 1.0343401432037354, koszt: 0.000123060304852398\nepoka: 80, theta: 1.0549986362457275, koszt: 4.584717916214815e-05\nepoka: 90, theta: 1.042211651802063, koszt: 1.859027886217772e-05\n\n\nJeszcze jeden przykład\n\nNapisz obwód kwantowy, który zawierać będzie bramkę \\(R_X\\) dla parametru \\(\\theta_1\\) oraz \\(R_Y\\) dla parametru \\(\\theta_2\\)\noblicz i uzasadnij, że wartość oczekiwana dla stanu \\(\\ket{\\psi} = R_Y(\\theta_2) R_X(\\theta_1) \\, \\ket{0}\\)\n\n\\[&lt;Z&gt;  = \\cos(\\theta_1) \\cos(\\theta_2)\\]\nMozliwe wartości średniej zawierają się w przedziale \\(-1\\), \\(1\\).\nPrzyjmij załozenie, ze optymalne rozwiązanie realizowane jest dla wartości oczekiwanej = 0.4\n\nimport pennylane as qml\nfrom pennylane import numpy as np \n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef par_c(theta):\n    qml.RX(theta[0], wires=0)\n    qml.RY(theta[1], wires=0)\n    return qml.expval(qml.PauliZ(0))\n\n\ndef cost_fn(theta):\n    return (par_c(theta) - 0.4)**2\n\ntheta = np.array([0.01, 0.02], requires_grad=True)\n\nopt = qml.GradientDescentOptimizer(stepsize=0.1)\n\nepochs = 100\n\nfor epoch in range(epochs):\n    theta = opt.step(cost_fn, theta)\n\n    if epoch % 10 == 0:\n        print(f\"epoka: {epoch}, theta: {theta}, koszt: {cost_fn(theta)}\")\n\nprint(f\"Optymalizacja zakonczona dla theta={theta}, koszt: {cost_fn(theta)}\")\n\nepoka: 0, theta: [0.01119924 0.02239872], koszt: 0.3596238551650218\nepoka: 10, theta: [0.03468299 0.06939827], koszt: 0.35640059384126277\nepoka: 20, theta: [0.10485556 0.21069384], koszt: 0.3277736421372642\nepoka: 30, theta: [0.26595847 0.55025891], koszt: 0.17843868824086426\nepoka: 40, theta: [0.41114867 0.91214351], koszt: 0.02593550926609833\nepoka: 50, theta: [0.45600131 1.05610411], koszt: 0.0017612620807984237\nepoka: 60, theta: [0.46619699 1.09390217], koszt: 0.00010074458607215528\nepoka: 70, theta: [0.4685347  1.10295946], koszt: 5.557697121461739e-06\nepoka: 80, theta: [0.469078   1.10508776], koszt: 3.040948516747214e-07\nepoka: 90, theta: [0.46920476 1.10558565], koszt: 1.6607272093790385e-08\nOptymalizacja zakonczona dla theta=[0.46923296 1.10569646], koszt: 1.2125189676042736e-09",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#klasyczne-dane",
    "href": "lectures/cwiczenia1.html#klasyczne-dane",
    "title": "Pennylane wprowadzenie",
    "section": "klasyczne dane",
    "text": "klasyczne dane\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\n\nN = 3\nwires = range(N)\ndev = qml.device('default.qubit', wires)\n\n\n@qml.qnode(dev)\ndef basis_encoding(features):\n    qml.BasisEmbedding(features, wires)\n    return qml.probs()\n\n\nbasis_encoding([1,1,1])\n\narray([0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\\[ \\ket{111} = \\ket{1}\\otimes \\ket{1} \\otimes \\ket{1} = [0 0 0 0 0 0 0 1]^T\\]\n\nbasis_encoding(7)\n\narray([0., 0., 0., 0., 0., 0., 0., 1.])\n\n\n\nn_wires = 4 \ndev = qml.device('default.qubit', wires= n_wires)\n\n@qml.qnode(dev)\ndef circ(features):\n    for i in range(len(features)):\n        if features[i] == 1:\n            qml.X(i)\n    qml.Barrier()\n    qml.Hadamard(1)\n    qml.CNOT([1,3])\n    return qml.state()\n\n\ncirc([1,0,1,0])\n\narray([0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j,\n       0.        +0.j, 0.        +0.j, 0.        +0.j, 0.        +0.j,\n       0.        +0.j, 0.        +0.j, 0.70710678+0.j, 0.        +0.j,\n       0.        +0.j, 0.        +0.j, 0.        +0.j, 0.70710678+0.j])\n\n\n\nqml.draw_mpl(circ, level='device', scale=0.7)([1,0,1,0])",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#amplitude-encoding",
    "href": "lectures/cwiczenia1.html#amplitude-encoding",
    "title": "Pennylane wprowadzenie",
    "section": "Amplitude encoding",
    "text": "Amplitude encoding\n\nimport pennylane as qml\nN = 3\nwires = range(N)\n\ndev = qml.device(\"default.qubit\", wires)\n\n@qml.qnode(dev)\ndef circuit(features):\n    qml.AmplitudeEmbedding(features, wires)\n    return qml.state()\n\n\ncircuit([0.625,0.0,0.0,0.0,0.625,0.375,0.25,0.125])\n\narray([0.625+0.j, 0.   +0.j, 0.   +0.j, 0.   +0.j, 0.625+0.j, 0.375+0.j,\n       0.25 +0.j, 0.125+0.j])\n\n\n\nimport pennylane as qml\nN = 3\nwires = range(N)\n\ndev = qml.device(\"default.qubit\", wires)\n\n@qml.qnode(dev)\ndef circuit(f=None):\n    qml.AmplitudeEmbedding(features=f, wires=dev.wires, normalize=True, pad_with=0)\n    return qml.expval(qml.PauliZ(0)), qml.state()\n\n\nvect = [0.1, -0.3, 0.5, 0.4, 0.2]\n\n\nnorm = np.linalg.norm(vect)\nnorm_vec = np.round([i / norm for i in vect], 4)\nprint(f\"Vec: {vect}, Norm{norm_vec}\")\n\nVec: [0.1, -0.3, 0.5, 0.4, 0.2], Norm[ 0.1348 -0.4045  0.6742  0.5394  0.2697]\n\n\n\nres, state = circuit(f=norm_vec)\nres2, state2 = circuit(f=vect)\n\n\nstate.real, state2.real\n\n(array([ 0.13479815, -0.40449446,  0.67419077,  0.53939262,  0.26969631,\n         0.        ,  0.        ,  0.        ]),\n array([ 0.13483997, -0.40451992,  0.67419986,  0.53935989,  0.26967994,\n         0.        ,  0.        ,  0.        ]))\n\n\n\nqml.draw_mpl(circuit)(norm_vec)\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\n\nX = data.data\ny = data.target\n\n\ndef prepare_ampl(x, target_len = 16):\n    padded = np.pad(x, (0, target_len - len(x)), mode=\"constant\")\n    normed = padded / np.linalg.norm(padded)\n    return np.array(normed, requires_grad=True)\n\n\nx0 = X[0]\nfeatures = prepare_ampl(x0)\n\n\nfeatures\n\ntensor([1.32644724e-02, 1.59397384e-03, 2.26512072e-03, 1.45415157e-02,\n        1.18382852e-01, 2.61001565e-03, 2.85237424e-03, 2.61001565e-04,\n        2.13461994e-03, 5.25731723e-03, 9.69434383e-04, 3.65402190e-03,\n        9.92738094e-01, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00], requires_grad=True)\n\n\n\nn_qubits = 4\ndev = qml.device('default.qubit', wires = n_qubits)\n\n@qml.qnode(dev)\ndef amplitude_circ(x):\n    qml.AmplitudeEmbedding(features=x, wires=range(n_qubits), normalize=False)\n    return qml.state()\n\n\nstate = amplitude_circ(features)\nstate\n\ntensor([1.32644724e-02+0.j, 1.59397384e-03+0.j, 2.26512072e-03+0.j,\n        1.45415157e-02+0.j, 1.18382852e-01+0.j, 2.61001565e-03+0.j,\n        2.85237424e-03+0.j, 2.61001565e-04+0.j, 2.13461994e-03+0.j,\n        5.25731723e-03+0.j, 9.69434383e-04+0.j, 3.65402190e-03+0.j,\n        9.92738094e-01+0.j, 0.00000000e+00+0.j, 0.00000000e+00+0.j,\n        0.00000000e+00+0.j], requires_grad=True)\n\n\n\n@qml.qnode(dev)\ndef amp_circ(x):\n    qml.AmplitudeEmbedding(features=x, wires=range(n_qubits), normalize=True, pad_with=0)\n    return qml.state()\n\n\nstate2 = amp_circ(X[0])\nstate2\n\narray([1.32644724e-02+0.j, 1.59397384e-03+0.j, 2.26512072e-03+0.j,\n       1.45415157e-02+0.j, 1.18382852e-01+0.j, 2.61001565e-03+0.j,\n       2.85237424e-03+0.j, 2.61001565e-04+0.j, 2.13461994e-03+0.j,\n       5.25731723e-03+0.j, 9.69434383e-04+0.j, 3.65402190e-03+0.j,\n       9.92738094e-01+0.j, 0.00000000e+00+0.j, 0.00000000e+00+0.j,\n       0.00000000e+00+0.j])",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#angle-encoding",
    "href": "lectures/cwiczenia1.html#angle-encoding",
    "title": "Pennylane wprowadzenie",
    "section": "Angle encoding",
    "text": "Angle encoding\n\\[ x \\to R_k(x) \\ket{0} = e^{-i\\,x \\frac{\\sigma_k}{2}} \\ket{0} \\]\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\nfeatures= [np.pi/3, np.pi/4]\ndev = qml.device('default.qubit', wires=2)\n\n@qml.qnode(dev)\ndef circ(features):\n    qml.AngleEmbedding(features=features, rotation='Y', wires=range(2))\n    return qml.probs(wires=[0,1])\n\n\nnp.round(circ(features), 3)\n\ntensor([0.64 , 0.11 , 0.213, 0.037], requires_grad=True)\n\n\n\nqml.draw_mpl(circ)(features)\n\n\n\n\n\n\n\n\n\nimport pennylane as qml\nimport pennylane.numpy as np\nfrom sklearn.preprocessing import normalize\nfrom sklearn.datasets import load_wine\n\ndata = load_wine()\n\nX = data.data\ny = data.target\n\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, np.pi))\nX_scaled = scaler.fit_transform(X)\n\n\nX_scaled[0]\n\narray([2.64555171, 0.60224207, 1.7975958 , 0.80968883, 1.94642154,\n       1.97162022, 1.80277047, 0.88913   , 1.86315274, 1.16871536,\n       1.43031861, 3.04953133, 1.76350458])\n\n\n\ndev = qml.device('default.qubit', wires=13)\n\n@qml.qnode(dev)\ndef emb(x):\n    qml.AngleEmbedding(x, wires=range(len(x)), rotation='Y')\n    return qml.expval(qml.PauliZ(0))\n\n\nemb(X_scaled[0])\n\nnp.float64(-0.8794737512064895)\n\n\n\n@qml.qnode(dev)\ndef emb(x):\n    qml.AngleEmbedding(x, wires=range(len(x)), rotation='Y')\n    return [qml.expval(qml.PauliZ(i)) for i in range(len(x))]\n\n\nemb(X_saled[0])\n\n[np.float64(-0.8794737512064895),\n np.float64(0.8240675736145868),\n np.float64(-0.2248601123708277),\n np.float64(0.6897237772781044),\n np.float64(-0.3668542188130566),\n np.float64(-0.39017706326055457),\n np.float64(-0.2298992328822939),\n np.float64(0.6300878435817112),\n np.float64(-0.28820944852718955),\n np.float64(0.3913341989876884),\n np.float64(0.14001614496862924),\n np.float64(-0.9957653484788057),\n np.float64(-0.1915177132878786)]",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#kernel-trick",
    "href": "lectures/cwiczenia1.html#kernel-trick",
    "title": "Pennylane wprowadzenie",
    "section": "Kernel trick",
    "text": "Kernel trick\nDla prawdziwych danych trudno oczekiwać aby były one liniowo separowalne.\nDlatego jednym z rozwiązań jest stworzenie odwzorowania do wyżej wymiarowej przestrzeni tak by dane w niej były już liniowo separowalne. Obliczenie takiej transformacji dla dowolnych danych jest bardzo trudne, dlatego możemy zastosować tzw kernel trick. Potrzebujemy tylko obliczyć iloczyn skalarny: \\[ K(x,x') = &lt;\\phi(x), \\phi(x')&gt;\\] bez jawnego wyznaczania \\(\\phi\\).\n\nx, x’ wektory wejściowe z oryginalnej przestrzeni\n\\(\\phi(x)\\) odwzorowanie do przestrzeni o wyższym wymiarze\n\\(K(x, x')\\) funkcja jądrowa - kernel function - oblicza iloczy skalarny w zadanej przestrzeni.\n\n\nLinear - \\(K(x, x') = x^{T}x'\\)\nPolynomial - \\(K(x,x') = (x^{T}x' +c)^d\\)\nRBF - \\(K(x,x') = exp(-\\gamma \\, |x-x'|^2)\\)\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.datasets import make_moons\nfrom sklearn.svm import SVC\n\nX,y = make_moons(n_samples=200, noise=0.2, random_state=42)\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=44)\n\npoly_svm_clf = Pipeline([\n  #  ('polu_features', PolynomialFeatures(degree=3)),\n    ('scaler', StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=1, loss='hinge'))\n])\n\npoly_svm_clf.fit(X_train, y_train)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('linear_svc', LinearSVC(C=1, loss='hinge'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\nsteps steps: list of tuples\n\nList of (name of step, estimator) tuples that are to be chained in\nsequential order. To be compatible with the scikit-learn API, all steps\nmust define `fit`. All non-last steps must also define `transform`. See\n:ref:`Combining Estimators ` for more details.\n[('scaler', ...), ('linear_svc', ...)]\n\n\n\ntransform_input transform_input: list of str, default=None\n\nThe names of the :term:`metadata` parameters that should be transformed by the\npipeline before passing it to the step consuming it.\n\nThis enables transforming some input arguments to ``fit`` (other than ``X``)\nto be transformed by the steps of the pipeline up to the step which requires\nthem. Requirement is defined via :ref:`metadata routing `.\nFor instance, this can be used to pass a validation set through the pipeline.\n\nYou can only set this if metadata routing is enabled, which you\ncan enable using ``sklearn.set_config(enable_metadata_routing=True)``.\n\n.. versionadded:: 1.6\nNone\n\n\n\nmemory memory: str or object with the joblib.Memory interface, default=None\n\nUsed to cache the fitted transformers of the pipeline. The last step\nwill never be cached, even if it is a transformer. By default, no\ncaching is performed. If a string is given, it is the path to the\ncaching directory. Enabling caching triggers a clone of the transformers\nbefore fitting. Therefore, the transformer instance given to the\npipeline cannot be inspected directly. Use the attribute ``named_steps``\nor ``steps`` to inspect estimators within the pipeline. Caching the\ntransformers is advantageous when fitting is time consuming. See\n:ref:`sphx_glr_auto_examples_neighbors_plot_caching_nearest_neighbors.py`\nfor an example on how to enable caching.\nNone\n\n\n\nverbose verbose: bool, default=False\n\nIf True, the time elapsed while fitting each step will be printed as it\nis completed.\nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\ncopy copy: bool, default=True\n\nIf False, try to avoid a copy and do inplace scaling instead.\nThis is not guaranteed to always work inplace; e.g. if the data is\nnot a NumPy array or scipy.sparse CSR matrix, a copy may still be\nreturned.\nTrue\n\n\n\nwith_mean with_mean: bool, default=True\n\nIf True, center the data before scaling.\nThis does not work (and will raise an exception) when attempted on\nsparse matrices, because centering them entails building a dense\nmatrix which in common use cases is likely to be too large to fit in\nmemory.\nTrue\n\n\n\nwith_std with_std: bool, default=True\n\nIf True, scale the data to unit variance (or equivalently,\nunit standard deviation).\nTrue\n\n\n\n\n            \n        \n    LinearSVC?Documentation for LinearSVC\n        \n            \n                Parameters\n                \n\n\n\n\n\n\n\n\n\npenalty penalty: {'l1', 'l2'}, default='l2'\n\nSpecifies the norm used in the penalization. The 'l2'\npenalty is the standard used in SVC. The 'l1' leads to ``coef_``\nvectors that are sparse.\n'l2'\n\n\n\nloss loss: {'hinge', 'squared_hinge'}, default='squared_hinge'\n\nSpecifies the loss function. 'hinge' is the standard SVM loss\n(used e.g. by the SVC class) while 'squared_hinge' is the\nsquare of the hinge loss. The combination of ``penalty='l1'``\nand ``loss='hinge'`` is not supported.\n'hinge'\n\n\n\ndual dual: \"auto\" or bool, default=\"auto\"\n\nSelect the algorithm to either solve the dual or primal\noptimization problem. Prefer dual=False when n_samples &gt; n_features.\n`dual=\"auto\"` will choose the value of the parameter automatically,\nbased on the values of `n_samples`, `n_features`, `loss`, `multi_class`\nand `penalty`. If `n_samples` &lt; `n_features` and optimizer supports\nchosen `loss`, `multi_class` and `penalty`, then dual will be set to True,\notherwise it will be set to False.\n\n.. versionchanged:: 1.3\nThe `\"auto\"` option is added in version 1.3 and will be the default\nin version 1.5.\n'auto'\n\n\n\ntol tol: float, default=1e-4\n\nTolerance for stopping criteria.\n0.0001\n\n\n\nC C: float, default=1.0\n\nRegularization parameter. The strength of the regularization is\ninversely proportional to C. Must be strictly positive.\nFor an intuitive visualization of the effects of scaling\nthe regularization parameter C, see\n:ref:`sphx_glr_auto_examples_svm_plot_svm_scale_c.py`.\n1\n\n\n\nmulti_class multi_class: {'ovr', 'crammer_singer'}, default='ovr'\n\nDetermines the multi-class strategy if `y` contains more than\ntwo classes.\n``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n``\"crammer_singer\"`` optimizes a joint objective over all classes.\nWhile `crammer_singer` is interesting from a theoretical perspective\nas it is consistent, it is seldom used in practice as it rarely leads\nto better accuracy and is more expensive to compute.\nIf ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\nwill be ignored.\n'ovr'\n\n\n\nfit_intercept fit_intercept: bool, default=True\n\nWhether or not to fit an intercept. If set to True, the feature vector\nis extended to include an intercept term: `[x_1, ..., x_n, 1]`, where\n1 corresponds to the intercept. If set to False, no intercept will be\nused in calculations (i.e. data is expected to be already centered).\nTrue\n\n\n\nintercept_scaling intercept_scaling: float, default=1.0\n\nWhen `fit_intercept` is True, the instance vector x becomes ``[x_1,\n..., x_n, intercept_scaling]``, i.e. a \"synthetic\" feature with a\nconstant value equal to `intercept_scaling` is appended to the instance\nvector. The intercept becomes intercept_scaling * synthetic feature\nweight. Note that liblinear internally penalizes the intercept,\ntreating it like any other term in the feature vector. To reduce the\nimpact of the regularization on the intercept, the `intercept_scaling`\nparameter can be set to a value greater than 1; the higher the value of\n`intercept_scaling`, the lower the impact of regularization on it.\nThen, the weights become `[w_x_1, ..., w_x_n,\nw_intercept*intercept_scaling]`, where `w_x_1, ..., w_x_n` represent\nthe feature weights and the intercept weight is scaled by\n`intercept_scaling`. This scaling allows the intercept term to have a\ndifferent regularization behavior compared to the other features.\n1\n\n\n\nclass_weight class_weight: dict or 'balanced', default=None\n\nSet the parameter C of class i to ``class_weight[i]*C`` for\nSVC. If not given, all classes are supposed to have\nweight one.\nThe \"balanced\" mode uses the values of y to automatically adjust\nweights inversely proportional to class frequencies in the input data\nas ``n_samples / (n_classes * np.bincount(y))``.\nNone\n\n\n\nverbose verbose: int, default=0\n\nEnable verbose output. Note that this setting takes advantage of a\nper-process runtime setting in liblinear that, if enabled, may not work\nproperly in a multithreaded context.\n0\n\n\n\nrandom_state random_state: int, RandomState instance or None, default=None\n\nControls the pseudo random number generation for shuffling the data for\nthe dual coordinate descent (if ``dual=True``). When ``dual=False`` the\nunderlying implementation of :class:`LinearSVC` is not random and\n``random_state`` has no effect on the results.\nPass an int for reproducible output across multiple function calls.\nSee :term:`Glossary `.\nNone\n\n\n\nmax_iter max_iter: int, default=1000\n\nThe maximum number of iterations to be run.\n1000\n\n\n\n\n            \n        \n    \n\n\n\nprint(f\"Test acc: {poly_svm_clf.score(X_test, y_test):.2f}\")\n\nTest acc: 0.82\n\n\n\nlinear_svm = SVC(kernel='linear', C=1)\nlinear_svm.fit(X_train, y_train)\ny_pred_linear = linear_svm.predict(X_test)\nacc_linear = accuracy_score(y_test, y_pred_linear)\n\n\nrbf_svm = SVC(kernel='rbf', C=1, gamma='scale')\nrbf_svm.fit(X_train, y_train)\ny_pred_rbf = rbf_svm.predict(X_test)\nacc_rbf = accuracy_score(y_test, y_pred_rbf)\n\n\ndef plot_decision_boundry(model, X, y, title):\n    h = 0.02\n    x_min, x_max = X[:, 0].min(), X[:, 0].max()+1\n    y_min, y_max = X[:, 1].min(), X[:, 1].max()+1\n\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k')\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()\n\n\nplot_decision_boundry(linear_svm, X_test, y_test, \"linear SVM\")\nplot_decision_boundry(rbf_svm, X_test, y_test, \"RBF SVM\")",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#idea-swap-testu",
    "href": "lectures/cwiczenia1.html#idea-swap-testu",
    "title": "Pennylane wprowadzenie",
    "section": "Idea swap testu",
    "text": "Idea swap testu\nSwap test służy do obliczania wartości\n\\[ |\\langle \\psi |\\phi \\rangle |^2 \\]\nczyli kwadratu modułu iloczynu skalarnego dwóch stanów kwantowych \\(|\\psi \\rangle\\) i \\(|\\phi \\rangle\\) .\n\n🔧 Obwód swap testu\nSwap test używa dodatkowego kubitu kontrolnego oraz bramki SWAP\nKontrolny kubit realizowany jest w stanie \\(|0\\rangle\\).\n\\[ \\ket{\\psi_0} = \\ket{0} \\otimes \\ket{\\psi} \\otimes \\ket{\\phi} \\]\n\n\n🎛️ Jak to działa\n\nZastosuj Hadamarda (zamiana bazy) na kontrolny (ancilla) kubit \\[ \\ket{\\psi_1} = (\\ket{0} + \\ket{1}) \\otimes \\ket{\\psi} \\otimes \\ket{\\phi} \\]\nZastosuj CSWAP (3 kubitowa bramka - controll = ancilla)\nZastosuj Hadamarda (powrót do bazy)\nPomiar ancilla kubitu.\n\nPrawdopodobieństwo, że kontrolny kubit da wynik \\(0\\), wynosi: \\[P(0)=\\frac{1+|\\langle \\psi |\\phi \\rangle |^2}{2}\\]\nPrawdopodobieństwo, że kontrolny kubit da wynik 1, wynosi: \\[P(1)=\\frac{1-|\\langle \\psi |\\phi \\rangle |^2}{2}\\]\nDzięki temu, mierząc kontrolny kubit, możemy wyznaczyć overlap między stanami.\n\nimport pennylane as qml\nimport pennylane.numpy as np\n\ndev_test = qml.device('default.qubit', wires=['ancilla','phi','psi'], shots=5000)\n\n@qml.qnode(dev_test)\ndef swap_test():\n    qml.Hadamard(wires='ancilla')\n    \n    qml.X(wires=['phi'])\n    qml.Hadamard(wires=['psi'])\n\n    qml.CSWAP(wires=['ancilla', 'phi', 'psi'])\n    qml.Hadamard(wires='ancilla')\n    return qml.sample(wires='ancilla')\n\nres = swap_test()\n\nprint(f\"P(0) = {np.mean(res==0)}, P(1) = {np.mean(res == 1)}\")\nprint(f\"{2*np.mean(res==0) - 1}\")\n\nweryfikacja\n\ndev = qml.device('default.qubit', wires=1)\n\n@qml.qnode(dev)\ndef phi():\n    qml.X(wires=0)\n    return qml.state()\n\n@qml.qnode(dev)\ndef psi():\n    qml.Hadamard(wires=0)\n    return qml.state()\n\ndef theory(phi, psi):\n    inner = np.vdot(phi, psi)\n    return float(np.abs(inner)**2)\n\ntheory(psi(), phi())",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  },
  {
    "objectID": "lectures/cwiczenia1.html#quantum-embedding",
    "href": "lectures/cwiczenia1.html#quantum-embedding",
    "title": "Pennylane wprowadzenie",
    "section": "Quantum Embedding",
    "text": "Quantum Embedding\nKwantowy Embedding reprezentuje klasyczne dane jako stan (wektor) w przestrzeni Hilberta. Odwzorowanie, które generuje embedding nazywamy quantum feature map.\nFeature map: \\(\\phi: X \\to F\\) gdzie \\(F\\) to nowa przestrzeń Hilberta stanów. \\[ x \\to \\ket{\\phi(x)} \\]\nW naszym przypadku to odwzorowanie realizują \\(U_{\\phi}(x)\\) macierze kodowania kątowego. \\[ \\ket{0} \\to U_{\\phi}(x)\\ket{0} \\]\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0, np.pi))\nX_scaled = scaler.fit_transform(X)\n\ny_scaled = 2 * y -1 \n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled)\n\nRozważmy model kwantowy w postaci: \\[\nf(x) = \\braket{\\phi(x) | M | \\phi{x} }\n\\]\nModel może być realizowany jako wariacyjny obwód kwantowy.\nZamiast jednak trenować parametry dla takiego obwodu możemy wykorzystać kwantowy kernel który realizuje się przez SWAP test.\nZamiast SWAP testu możemy wykorzystać inny obwód Szczegóły tutaj\n\nfrom pennylane.templates import AngleEmbedding\n\n\nn_qubits = 2\ndev_kernel = qml.device('lightning.qubit', wires= n_qubits)\n\n\n\nprojector = np.zeros((2 ** n_qubits, 2 ** n_qubits))\nprojector[0, 0] = 1\n\n\n@qml.qnode(dev_kernel)\ndef kernel(x1, x2):\n    AngleEmbedding(x1, wires=range(n_qubits))\n    qml.adjoint(AngleEmbedding)(x2, wires=range(n_qubits))\n    return qml.expval(qml.Hermitian(projector, wires=range(n_qubits)))\n\n\nimport pennylane.numpy as np \n\n\nX_train = np.array(X_train, requires_grad=False)\nX_test = np.array(X_test, requires_grad=False)\n\ny_train = np.array(y_train, requires_grad=False)\ny_test = np.array(y_test, requires_grad=False)\n\n\nkernel(X_train[0], X_train[0]), kernel(X_test[0], X_test[1])\n\n(array(1.), array(0.85153324))\n\n\n\ndef kernel_matrix(A, B):\n    return np.array([[kernel(a,b) for b in B] for a in A])\n\nsvm = SVC(kernel=kernel_matrix).fit(X_train, y_train)\n\n\npredictions = svm.predict(X_test)\n\n\nprint(f\"model qsvm {accuracy_score(predictions, y_test):.4f}\")\n\nmodel qsvm 0.8400\n\n\n\nsvm.predict(X_test[:4]), y_test[:4]\n\n(array([-1, -1, -1, -1]), tensor([-1, -1, -1, -1], requires_grad=False))",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Pennylane wprowadzenie"
    ]
  }
]
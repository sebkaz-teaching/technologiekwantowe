[
  {
    "objectID": "lectures/wyklad1.html",
    "href": "lectures/wyklad1.html",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "",
    "text": "Nature isn’t classical, dammit, and if you want to make a simulation of Nature, you’d better make it quantum mechanical, and by golly it’s a wonderful problem because it doesn’t look so easy.\nRichard Feynman\nCelem tego wykładu jest zrozumienie, czym są:\nQuntum Machine Learning",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#uczenie-maszynowe",
    "href": "lectures/wyklad1.html#uczenie-maszynowe",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "Uczenie maszynowe",
    "text": "Uczenie maszynowe\nUczenie maszynowe (ale również AI i uczenie głębokie) to nauka i ,,sztuka’’ opisująca jak sprawić by komputery mogły ,,uczyć się’’ na podstawie danych, tak by rozwiązać problemy, których typowe programowanie nie miałoby sensu (lub byłoby zbyt skomplikowane).\n\nIn 1959, Arthur Samuel:\na field of study that gives computers the ability to learn without being explicitly programmed.\n\n\nDane\n\n\n\nModele\n\n\nUczenie nadzorowane (ang. supervised learning) - posiadając oznaczone dane \\((x_i, y_i)\\) szukamy funkcji \\(f(x_i) = y_i\\) tak by uogólnić ją na nowe dane. Np. dać kredyt, czy na obrazie jest kot albo pies itp.\nUczenie nienadzorowane (ang. unsupervised learning) - posiadając dane \\((x_i)\\) szukamy ukrytych struktur w danych.\nUczenie przez wzmacnianie (ang. reinforcement learning) - agent uczy się realizować zadania w środowisku na podstawie nagród i kar.\n\nJednym z podstawowych celów uczenia maszynowego (a takze i głębokiego), jest przypisanie klasy (target, labels) dla nowych, nieoznakowanych danych.\nIstnieją dwa główne typy dla tego zadania:\n\nRegresja – przewidywanie wartości ciągłej,\nklasyfikacja - przewidywanie wartości dyskretnej.\n\n\n\nFunkcja straty\nFunkcja straty mierzy jak przewidywania modelu są oddalone od rzeczywistych wartości.\n\nPomaga optymalizować parametry modelu przez mechanizm propagacji wstecz\nPozwala na dopasowanie modelu do danych\nMniejsza wartość funkcji straty = lepsza jakość modelu.\n\n\n\n\n\nSieci Neuronowe\n\n\nModel z parametrami do trenowania \\(f(x;\\theta)= \\sigma(Wx+b)\\) gdzie \\(\\theta= \\{W, b\\}\\)\nFunkcja kosztu \\(C = \\sum_{i} ( f(x_i, \\theta)-y_i )^2\\)\nSpadek po gradiencie\n\noblicz gradient funkcji kosztu\nzaktualizuj parametry \\(\\theta^{t+1} = \\theta^{t} - \\eta \\nabla C\\)\n\n\n\nimport torch\nfrom torch.autograd import Variable\n\ndata = torch.tensor([(0. , 1.), (0.1 , 1.1), (0.2 , 1.2)])\n\ndef model(phi, x=None):\n    return x*phi\n\ndef loss(a, b):\n    return torch.abs(a-b) ** 2\n\ndef avg_loss(phi):\n    c = 0 \n    for x, y in data:\n        c += loss(model(phi, x=x), y)\n    return c\n\nphi_ = Variable(torch.tensor(0.1), requires_grad=True)\nopt = torch.optim.Adam([phi_],lr=0.2)\n\nfor i in range(5):\n    l = avg_loss(phi_)\n    print(f\"cost: {l}, for phi: {phi_}\")\n    l.backward()\n    opt.step()\n\ncost: 3.5805001258850098, for phi: 0.10000000149011612\ncost: 3.44450044631958, for phi: 0.29999998211860657\ncost: 3.3168272972106934, for phi: 0.49334585666656494\ncost: 3.1936793327331543, for phi: 0.6854467988014221\ncost: 3.073840856552124, for phi: 0.878169059753418\n\n\n\n\nObliczenia kwantowe opisują przetwarzenie informacji na urządzeniach pracujących zgodnie z zasadami mechaniki kwantowej.\n\nUwaga! klasyczne komputery (tranzystory) również działają zgodnie z zasadami mechaniki kwantowej, ale wykonywane operacje opierają się o logikę klasyczną.\n\nOba kierunki są istotne w procesie przetwarzania danych obecnie i w niedalekiej przyszłości. Dlatego naturalnym pytaniem jest jak je ze sobą połączyć?\nQML to realizowanie metod uczenia maszynowego, które mogą być wykonywane na komputerach kwantowych.\nKwantowe uczenie maszynowe możemy określić jako uczenie maszynowe realizowane na komputerach kwantowych. Zasadniczym jest pytanie na ile i czy wogóle komputery kwantowe mogą poprawić jakość modeli uczenia maszynowego i czy pozwalają zrealizować coś więcej niż wykorzystanie klasycznych komputerów.\n\n\nHistoria MK\nPoczątek Mechaniki Kwantowej związane są z pracami Maxa Plancka (1900) i Alberta Einsteina (1905), którzy wprowadzili pojęcie kwantu - czyli najmniejszej porcji energii. Dalszy rozwój Mechaniki Kwantowej związany jest z badaniami takich naukowców jak Niels Bohr, Erwin Schrödinger, Louis de Broglie, Heisenberg, Dirac, Feynman i wielu innych. Pozostałe informacje możesz znaleźć w artykule o obliczeniach kwantowych\n\nInformatyków (najczęściej) nie interesuje, w jaki sposób właściwości fizyczne układów są wykorzystywane do przechowywania informacji w komputerze klasycznym. Podobnie, nie muszą się zastanawiać nad fizycznym mechanizmem, za pomocą którego informacja kwantowa jest realizowana w komputerze kwantowym. Czy prowadząc samochód zastanawiasz się, jak dokładnie działają wszystkie jego części? A pisząc kod modelu, zastanawiasz się, jak został on zaimplementowany w bibliotece?” Informatycy często nie muszą zagłębiać się w szczegóły fizycznej realizacji, skupiając się za to na wydajnym wykorzystaniu technologii komputerowych.\n\n\n\nHistoria obliczeń kwantowych\n\n1936 Alan Turing opublikował pracę On Computable Numbers, która stanowiła istotny krok w kierunku teoretycznych podstaw obliczeń (Hilbert Problems) - universal computing machine local\n1976 Roman S. Ingarden - Quantum Information Theory Roman S. Ingarden wprowadził pojęcie teorii informacji kwantowej, co miało kluczowe znaczenie dla rozwoju komputerów kwantowych.\n1980 Paul Benioff - Paul Benioff przedstawił teoretyczną koncepcję komputerów kwantowych jako fizycznych systemów, otwierając drzwi do praktycznych implementacji.\n1981 Richard Feynman - zwrócił uwagę na to, że klasyczne komputery nie są w stanie efektywnie symulować procesów kwantowych.\n1985 David Deutsch opracował pierwszy opis kwantowej maszyny Turinga i algorytmów przeznaczonych do uruchamiania na komputerach kwantowych, w tym bramek kwantowych.\n1994 Peter Shor opracował algorytm faktoryzacji liczb w czasie wielomianowym, co miało znaczenie dla kryptografii i bezpieczeństwa informacji.\n1996 Lov Grover - Lov Grover stworzył algorytm Grover’a, który okazał się wyjątkowo efektywny w przeszukiwaniu stanów kwantowych.\n2000 Został zbudowany pierwszy komputer kwantowy (5 qubitów) oparty na nuklearnym rezonansie magnetycznym, co stanowiło ważny krok w rozwoju fizycznych platform komputerów kwantowych.\n2001 Demonstracja algorytmu Shora potwierdziła praktyczność i znaczenie algorytmów kwantowych.\n2007 Firma D-Wave dokonała pierwszej sprzedaży komercyjnego komputera kwantowego, co miało wpływ na rozwój technologii komputerów kwantowych w sektorze prywatnym.\nFirma IBM dokonała znaczącego przełomu, pokazując, że klasyczne superkomputery nie są w stanie efektywnie symulować systemów zawierających więcej niż 56 kubitów, co jest znane jako “quantum supremacy.”\n23 października 2019: Google ogłosił uzyskanie tzw. quantum supremacy na 53 kubitach.\n2020 Zespół Jian-Wei Pana z University of Science and Technology of China dokonał przełomu, realizując 76 fotonowych kubitów na komputerze Jiuzhang.\n2023 Pierwszy logiczny qubit?\n2025 Google Quantum Echoes\n\nOd około 1990 roku fizycy i informatycy pracują nad fizyczną realizacją komputerów kwantowych. Jednym z popularnych modeli obliczeń na komputerach kwantowych jest model oparty na kwantowych obwodach (ang. quantum circuit), który wykorzystuje qubity zamiast klasycznych bitów. Podobnie jak w przypadku obwodów klasycznych, w modelu kwantowym definiuje się bramki kwantowe (ang. quantum gates), które pozwalają na wykonywanie operacji na qubitach.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#dlaczego-chcemy-używać-komputerów-kwantowych",
    "href": "lectures/wyklad1.html#dlaczego-chcemy-używać-komputerów-kwantowych",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "Dlaczego chcemy używać komputerów kwantowych?",
    "text": "Dlaczego chcemy używać komputerów kwantowych?\n\nKwantowa złożoność (Quantum Complexity)\nNowy paradygmat wykorzystuje unikalne cechy interferencji, superpozycji i splątania w celu wykonywania obliczeń.\nObecnie realizowany jest w trzech modelach:\n\nQuantum Circuits (Obwody Kwantowe) - oparty na modelu bramkowym, pozwala realizować algorytmy typu QAOA, VQA, oraz metody hybrydowe.\nAdiabatyczne Obliczenia Kwantowe (D-Wave) - polegające na minimalizacji energii, z wykorzystaniem optymalizacji QUBO i analogii do modelu Isinga.\nTopologiczne Komputery Kwantowe - oparte na topologicznych kubitach.\n\nProblemy, które uważamy za trudne do rozwiązania klasycznie, takie jak optymalizacja, stają się łatwiejsze dla komputerów kwantowych. Przykładem moze być faktoryzacja liczb. Klasyczne komputery nie są w stanie efektywnie symulować działania kwantowych komputerów. Koszt najlepszych symulatorów rośnie wykładniczo wraz z liczbą kubitów. Możliwości komputerów kwantowych są potencjalnie ogromne, ale obecnie istnieją pewne ograniczenia link. Kwantowy komputer może być używany do efektywnej symulacji niemal dowolnego procesu fizycznego zachodzącego w przyrodzie, choć nie zawsze jesteśmy pewni, czy taka symulacja jest możliwa.\nPodstawowym faktem przewagi komputerów kwantowych nad klasycznymi jest tzw. parallelizm. Ze względu, iż kubity moga znajdowac się w superpozycji stanów, komputer kwantowy może przeprowadzic obliczenia jednocześnie na wszystkich stanach. Co dokładnie to oznacza, poznamy w dalszej czesci wykładu. Rozważmy sytuację w której chcemy poznac działanie funkcji \\(f(x)\\) dla pewnego argumentu \\(x\\) (dla pewnej liczby). Aby znaleźc wynik dla dwóch liczb (np. \\(x=0\\) i \\(x=1\\)) klasyczny komputer musi wykonac dwie operacje. Komputer kwantowy może uzuskac ten wynik przeprowadzajac obliczenia jednocześnie dla obu warości. Do wykonania takiej operacji wystarczy jeden kubit. Następnie jeżeli będziemy chcieli obliczyc nasza funkcję dla kolejnych liczb \\(x=2\\) (która binarnie reprezentowana jest jako \\(10\\)) oraz liczby \\(x=3\\) (binarnie \\(11\\)) musimy dodac kolejny (jeden!) kubit. Dwa kubity moga posłużyc do realizacji czterech równoległych operacji. Jeśli rozważymy 3 kubity znowu mozemy podwoic ilośc operacji (3 kubity maja 8 stanów bazowych). Dodanie kubitu do komputera kwantowego pozwala podwoic liczbę obliczeń. W przypadku klasycznego komputera aby uzyskac taki efekt, potrzeba podwoic rownież liczbę bitów. n-kubitów moze realizowac \\(2^n\\) równoległych obliczeń.\n\n\nKwantowa korekcja błędów (Quantum Error Correction)\nDekoherencja, czyli oddziaływanie z otoczeniem, niszczy stan komputera kwantowego i wprowadza błędy obliczeniowe. Istnieje potrzeba zabezpieczenia przed tym zjawiskiem. Obliczenia kwantowe wymagają tzw. korekcji błędów, która pomaga w utrzymaniu integralności obliczeń na komputerach kwantowych. Aktualnie mówimy o erze Noisy Intermediate-Scale Quantum (NISQ), co oznacza, że komputery kwantowe wciąż potrzebują rozwoju w zakresie korekcji błędów i stabilności.\n\n\nRealizacja fizyczna komputerów kwantowych\nprocesory kwantowe\n\n\n\nProces obliczeń kwantowych\nWykonanie obliczeń związane jest z pojęciem fizycznego doświadczenia. Będzie się ono składać z trzech części:\n\nprzygotowanie (przygotuj stan kwantowy kubitów),\newolucja (przeprowadź transformację za pomocą bramek kwantowych),\npomiar i interpretacja wyników.\n\n\nPodobnie w informatyce i w analizach danych wykonujemy obliczenia klasyczne. przygotowujemy dane (stan początkowy); następnie wykonujemy program (ewolucja) i odczytujemy wyniki (pomiar).\n\nNie obserwujemy tych etapów podczas codziennej interakcji z komputerem, więc nie zauważamy w sposób świadomy powyższego schematu działania. Piotr Gawron, Oscar Słowik - Rewolucja Stanu, Fantastyczne wprowadzenie do informatyki kwantowej.\n\nKażdy komputer kwantowy (koprocesor) musi komunikować się z podukładem klasycznym.",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "lectures/wyklad1.html#quantum-machine-learning",
    "href": "lectures/wyklad1.html#quantum-machine-learning",
    "title": "Wprowadzenie do obliczeń kwantowych",
    "section": "Quantum Machine Learning",
    "text": "Quantum Machine Learning\n\nKlasyczne dane w kwantowym uczeniu maszynowym\n\n\nCC - Classical data using classical computers, algorytmy inspirowane obliczeniami kwantowymi\nQC - Quantum data using classical (ML) computers. link1, link2, link3\nCQ - Classical data on qunatum computers. Na tym chcemy się skupić.\nQQ - Quantum data on quantum computers. Who knows?\n\n\n\nDostęp do obliczeń kwantowych w chmurze\n\nIBM Quantum z wykorzystaniem biblioteki qiskit.\nPennylane z wykorzystaniem biblioteki pennylane.\nCirq Google z wykorzystaniem biblioteki cirq.\nD-Wave - Python\nXanadu - Pennylane Python library\nAmazon braket - AWS Python, Julia",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Wprowadzenie do obliczeń kwantowych"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Podstawowe informacje znajdziesz w sylabusie.\nCiekawe książki i strony internetowe zamieszczone zostały w zakładce książki. Jeśli chciał(a)byś coś dodać prześlij informację przez MS teams.",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Informacje ogólne",
    "section": "",
    "text": "Podstawowe informacje znajdziesz w sylabusie.\nCiekawe książki i strony internetowe zamieszczone zostały w zakładce książki. Jeśli chciał(a)byś coś dodać prześlij informację przez MS teams.",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#kalendarz",
    "href": "index.html#kalendarz",
    "title": "Informacje ogólne",
    "section": "Kalendarz",
    "text": "Kalendarz\n\nWykład",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#technologie",
    "href": "index.html#technologie",
    "title": "Informacje ogólne",
    "section": "Technologie",
    "text": "Technologie\nUczestnicząc w zajęciach musisz opanować i przynajmniej w podstawowym zakresie posługiwać się następującymi technologiami informatycznymi:\n\nAlgebra liniowa - wektory, macierze, baza, iloczyn skalarny, iloczyn tensorowy\nPython, Jupyter notebook, Jupyter lab, Colab\nAlgorytmy sieci neuronowych i uczenia maszynowego w procesie klasyfikacji binarnej",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "index.html#qpoland",
    "href": "index.html#qpoland",
    "title": "Informacje ogólne",
    "section": "QPoland",
    "text": "QPoland\nQPoland jest cześcią międzynarodowej sieci QWorld.\nZapraszamy do wspólnego poszerzania wiedzy. Szczegóły &gt; QWorld is a global network of individuals, groups, and communities collaborating on education and implementation of quantum technologies and research activities.",
    "crumbs": [
      "Książki",
      "Informacje ogólne"
    ]
  },
  {
    "objectID": "ksiazki.html",
    "href": "ksiazki.html",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "Chris Bernhardt, Obliczenia kwantowe dla każdego. PWN 2020\n\n\nWyjaśnienie jak to działa w obliczeniach kwantowych.\n\n\nMichel Le Bellac, Wstęp do informatyki kwantowej. PWN 2011\n\n\nTrudniejsza, dużo matematyki i fizyki.\n\n\nThomas G. Wong, Introduction to Classical and Quantum Computing. Rooted Grove 2022.\n\n\nBardzo dobra!, dużo przykładów, dużo ciekawych informacji wyjaśnianych bardzo szczegółowo.\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nP. Gawron, M. Cholewa, … Rewolucja stanu. Fantastyczne wprowadzenie do informatyki kwantowej. Quantumz.io 2021\nA. Saxena, J. Mancilla, I. Montalban, C. Pere, Financial Modeling Using Quantum Computing. Packt 2023\nM. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Springer 2021.\n\n\n\n\n\n\n\nL. Moroney, Sztuczna inteligencja i uczenie maszynowe dla programistów. Praktyczny przewodnik po sztucznej inteligencji. Helion 2021. Zobacz opis lub Kup e-book\nBruce, Bruce, Gedeck, Statystyka praktyczna w data science. Wydanie II. Helion. 2021."
  },
  {
    "objectID": "ksiazki.html#książki",
    "href": "ksiazki.html#książki",
    "title": "Książki i strony WWW",
    "section": "",
    "text": "Chris Bernhardt, Obliczenia kwantowe dla każdego. PWN 2020\n\n\nWyjaśnienie jak to działa w obliczeniach kwantowych.\n\n\nMichel Le Bellac, Wstęp do informatyki kwantowej. PWN 2011\n\n\nTrudniejsza, dużo matematyki i fizyki.\n\n\nThomas G. Wong, Introduction to Classical and Quantum Computing. Rooted Grove 2022.\n\n\nBardzo dobra!, dużo przykładów, dużo ciekawych informacji wyjaśnianych bardzo szczegółowo.\n\n\nA. Jacquier, O. Kondratyev, Quantum Machine Learning and Optimisation in Finance. On the Road to Quantum Advantage.\nP. Gawron, M. Cholewa, … Rewolucja stanu. Fantastyczne wprowadzenie do informatyki kwantowej. Quantumz.io 2021\nA. Saxena, J. Mancilla, I. Montalban, C. Pere, Financial Modeling Using Quantum Computing. Packt 2023\nM. Schuld, F. Petruccione, Machine Learning with Quantum Computers, Springer 2021.\n\n\n\n\n\n\n\nL. Moroney, Sztuczna inteligencja i uczenie maszynowe dla programistów. Praktyczny przewodnik po sztucznej inteligencji. Helion 2021. Zobacz opis lub Kup e-book\nBruce, Bruce, Gedeck, Statystyka praktyczna w data science. Wydanie II. Helion. 2021."
  },
  {
    "objectID": "ksiazki.html#strony-www",
    "href": "ksiazki.html#strony-www",
    "title": "Książki i strony WWW",
    "section": "Strony WWW",
    "text": "Strony WWW\n\nPeter Shor Wykład\n\n\nPakiety Python\n\nQiskit\nPennyLane\n\n\n\nPakiety Julia\n\nYao\nQAOA\n\n\n\nEdytory tekstu\n\nNotepad++\nSublime Text\nVisual Studio Code\n\n\n\nMarkdown\n\nMD\n\n\n\nFilmy\n\nWprowadzenie do obliczeń kwantowych\nQPoland, Bronze, Warsztaty z programowania komputerów kwantowych 2023"
  },
  {
    "objectID": "info.html",
    "href": "info.html",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3 -m venv &lt;name of env&gt;\n\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n\n(venv)$ \nJak uruchomić środowisko pythona w systemie Windows.\nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#python",
    "href": "info.html#python",
    "title": "Narzędzia",
    "section": "",
    "text": "W terminalu (Windows CMD) wpisz\npython\nJeśli nie odnaleziono komendy uruchom polecenie:\npython3\nZwróć uwagę, aby Twoja wersja nie była niższa niż 3.X Aby wyjść z powłoki pythona użyj funkcji exit()\nPython 3.10.9 (main, Dec 15 2022, 17:11:09) [Clang 14.0.0 (clang-1400.0.29.202)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; exit()\n\n\npython3 -m venv &lt;name of env&gt;\n\nsource &lt;name of env&gt;/bin/activate\n# . env/bin/activate\n\n(venv)$ \nJak uruchomić środowisko pythona w systemie Windows.\nSzybka instalacja podstawowych bibliotek i jupyterlab.\npip install --no-cache --upgrade pip setuptools\n\npip install jupyterlab numpy pandas matplotlib scipy\n# jeśli masz plik requirements.txt z potrzebnymi bibliotekami\npip install -r requirements.txt\n# uruchom \njupyterlab\nW przeglądarce internetowej wpisz: localhost:8888\nPo ponownym uruchomieniu przejdź do katalogu w którym utworzyłeś środowisko, następnie uruchom środowisko i jupyterlab.\nsource &lt;name of env&gt;/bin/activate\njupyterlab\n\n\n\nKurs podstaw pythona Tomas Beuzen polecam.\nUtwórz konto na Kaggle, przejdź do zakładki Courses i przerób cały moduł Pythona. Zawiera on:\n\nwyrażenia i zmienne\nfunkcje\nwarunki i flow programu\nlisty\npętle\nstringi i słowniki\ndodawanie i używanie zewnętrznych bibliotek"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-serwisu-github",
    "href": "info.html#zacznij-korzystać-z-serwisu-github",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z serwisu GitHub",
    "text": "Zacznij korzystać z serwisu GitHub\n\n\n\nTekst na podstawie strony jak korzystać z serwisu github\nPracując nad projektem np. praca magisterska, (samodzielnie lub w zespole) często potrzebujesz sprawdzić jakie zmiany, kiedy i przez kogo zostały wprowadzone do projektu. W zadaniu tym świetnie sprawdza się system kontroli wersji czyli GIT.\nGit możesz pobrać i zainstalować jak zwykły program na dowolnym komputerze. Jednak najczęściej (małe projekty) korzysta się z serwisów z jakimś systemem git. Jednym z najbardziej rozpoznawanych jest GitHub dzięki któremu możesz korzystać z systemu git bez jego instalacji na swoim komputerze.\nW darmowej wersji serwisu GitHub swoje pliki możesz przechowywać w publicznych (dostęp mają wszyscy) repozytoriach.\nSkupimy się wyłącznie na darmowej wersji serwisu GitHub.\ngit --version\n\nStruktura GitHuba\nNa najwyższym poziomie znajdują się konta indywidualne (np http://github.com/sebkaz, bądź zakładane przez organizacje. Użytkownicy indywidualni mogą tworzyć repozytoria publiczne (public ) bądź prywatne (private).\nJeden plik nie powinien przekraczać 100 MB.\nRepo (skrót do repozytorium) tworzymy za pomocą Create a new repository. Każde repo powinno mieć swoją indywidualną nazwę.\n\n\nBranche\nGłówna (tworzona domyślnie) gałąź rapozytorium ma nazwę master.\n\n\nNajważniejsze polecnia do zapamiętania\n\nściąganie repozytorium z sieci\n\ngit clone https://adres_repo.git\n\nW przypadku githuba możesz pobrać repozytorium jako plik zip.\n\n\nTworzenie repozytorium dla lokalnego katalogu\n\n# tworzenie nowego katalogu\nmkdir datamining\n# przejście do katalogu\ncd datamining\n# inicjalizacja repozytorium w katalogu\ngit init\n# powinien pojawić się ukryty katalog .git\n# dodajmy plik\necho \"Info \" &gt;&gt; README.md\n\nPołącz lokalne repozytorium z kontem na githubie\n\ngit remote add origin https://github.com/&lt;twojGit&gt;/nazwa.git\n\nObsługa w 3 krokach\n\n# sprawdź zmiany jakie zostały dokonane\ngit status\n# 1. dodaj wszystkie zmiany\ngit add .\n# 2. zapisz bierzący stan wraz z informacją co zrobiłeś\ngit commit -m \" opis \"\n# 3. potem już zostaje tylko\ngit push origin master\nWarto obejrzeć Youtube course.\nCiekawe i proste wprowadzenie mozna znaleźć tutaj"
  },
  {
    "objectID": "info.html#zacznij-korzystać-z-dockera",
    "href": "info.html#zacznij-korzystać-z-dockera",
    "title": "Narzędzia",
    "section": "Zacznij korzystać z Dockera",
    "text": "Zacznij korzystać z Dockera\n\n\n\nW celu pobrania oprogramowania docker na swój system przejdź do strony.\nJeżli wszystko zainstalowało się prawidłowo wykonaj następujące polecenia:\n\nSprawdź zainstalowaną wersję\n\ndocker --version\n\nŚciągnij i uruchom obraz Hello World i\n\ndocker run hello-world\n\nPrzegląd ściągnietych obrazów:\n\ndocker image ls\n\ndocker images\n\nPrzegląd uruchomionych kontenerów:\n\ndocker ps \n\ndocker ps -all\n\nZatrzymanie uruchomionego kontenera:\n\ndocker stop &lt;CONTAINER ID&gt;\n\nUsunięcie kontenera\n\ndocker rm -f &lt;CONTAINER ID&gt;\nPolecam również krótkie intro"
  },
  {
    "objectID": "lectures/kodyw1.html",
    "href": "lectures/kodyw1.html",
    "title": "Modele uczenia maszynowego",
    "section": "",
    "text": "Do wygenerowania kodów użyjemy biblioteki PyTorch",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/kodyw1.html#regresja-liniowa",
    "href": "lectures/kodyw1.html#regresja-liniowa",
    "title": "Modele uczenia maszynowego",
    "section": "Regresja liniowa",
    "text": "Regresja liniowa\nWygenerujemy niezaszumione dane na podstawie wzoru \\(y = 2 x - 1\\). Na podstawie zbioru danych postaramy się oszacować nieznane parametry czyli wyraz przy \\(x\\) (\\(\\alpha_1 = 2\\)) i wyraz wolny (\\(\\alpha_0 = -1\\)).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# zbior danych\nx = range(11)\ny = [2*xi - 1 for xi in x]\nplt.plot(x, y, 'go', label='True data', alpha=0.5)\n\n\n\n\n\n\n\n\nModel regresji liniowej dla jednej zmiennej można zrealizować jako prostą jednowarstwową sieć neuronową. Cały proces można zrealizować za pomocą obiektu torch.nn.Linear\n\nimport torch\n\nclass LinearRegression(torch.nn.Module):\n\n    def __init__(self, inputSize, outputSize):\n        super(LinearRegression, self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(inputSize, outputSize)\n        ) \n        \n    def forward(self, x):\n        return self.layers(x)\n\n\nm = torch.nn.Linear(1,1)\n\n\nm\n\nLinear(in_features=1, out_features=1, bias=True)\n\n\n\nm.weight, m.bias\n\n(Parameter containing:\n tensor([[-0.1154]], requires_grad=True),\n Parameter containing:\n tensor([0.4927], requires_grad=True))\n\n\nAby nasze dane mogłybyć przeliczane przez bibliotekę PyTorch musimy je przetworzyć na tensory - czyli obiekty z biblioteki PyTorch.\n\n# dostosowanie do pytorch\nx = np.array(x, dtype=np.float32)\ny = np.array(y, dtype=np.float32)\n\nX_train = torch.from_numpy(x).view(-1,1)\ny_train = torch.from_numpy(y).view(-1,1)\n\nUwaga - ponieważ mamy jedną zmienną zawierającą 10 przypadków - potrzebujemy listy składającej się z 10 list jednoelementowych.\nMozna tez wykorzystac obiektowe programowanie.\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass LinearDataset(Dataset):\n    def __init__(self, X_train, y_train):\n        self.X_train = X_train # tensor typu torch\n        self.y_train = y_train\n\n    def __len__(self):\n        return len(self.y_train)\n\n    def __getitem__(self, idx):\n        return self.X_train[idx], self.y_train[idx]\n\n\ndataset = LinearDataset(X_train=X_train, y_train=y_train)\n\n\ndataloader = DataLoader(dataset, shuffle=True, batch_size=2)\n\nMożemy utworzyć model i wybrać optymalizator z funkcją kosztu.\n\n# obiekt liniowej regresji w wersji sieci nn\nlr_model = LinearRegression(1,1)\n\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(lr_model.parameters(), lr=0.01)\n\nMożemy sprawdzić, że nasz model będzie dostrajał 2 parametry.\n\nnum_params = sum(p.numel() for p in lr_model.parameters() if p.requires_grad)\nprint(f\"liczba trenowalnych parametrów: {num_params}\")\n\nliczba trenowalnych parametrów: 2\n\n\nParametry te w początkowej inicjalizacji mają następujące wartości:\n\nfor layer in lr_model.layers:\n    if isinstance(layer, torch.nn.Linear):\n        print(f\"weight: {layer.state_dict()['weight']}\")\n        print(f\"bias: {layer.state_dict()['bias']}\")\n\nweight: tensor([[0.7394]])\nbias: tensor([0.1832])\n\n\n\nepochs = 400\n# petla uczaca \nfor epoch in range(epochs):\n    lr_model.train() # etap trenowania \n\n    y_pred = lr_model(X_train)\n    loss = criterion(y_pred, y_train)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1:03d}, loss = {loss.item():.4f}')\n \n    lr_model.eval() # etap ewaluacji modelu\n\n# po treningu jeszcze raz generujemy predykcje\nlr_model.eval()\nwith torch.no_grad():\n    predicted = lr_model(X_train)\n\nepoch: 050, loss = 0.2946\nepoch: 100, loss = 0.1681\nepoch: 150, loss = 0.0959\nepoch: 200, loss = 0.0547\nepoch: 250, loss = 0.0312\nepoch: 300, loss = 0.0178\nepoch: 350, loss = 0.0101\nepoch: 400, loss = 0.0058\n\n\nMozna tez wykorzystac obiekt dataloader\n\nfor epoch in range(50):\n    for X_batch, y_batch in dataloader:\n        preds = lr_model(X_batch)\n        loss = criterion(preds, y_batch)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    if epoch % 10 == 0:\n        print(f\"Epoch {epoch}, loss = {loss.item():.4f}\")\n\nEpoch 0, loss = 0.0060\nEpoch 10, loss = 0.0046\nEpoch 20, loss = 0.0000\nEpoch 30, loss = 0.0014\nEpoch 40, loss = 0.0001\n\n\nOtrzymane parametry po uczeniu\n\nprint(f\"po procesie uczenia waga: {lr_model.layers[0].weight} oraz bias {lr_model.layers[0].bias}\")\n\npo procesie uczenia waga: Parameter containing:\ntensor([[1.9952]], requires_grad=True) oraz bias Parameter containing:\ntensor([-0.9755], requires_grad=True)\n\n\nDopasowanie modelu do danych można przedstawić na wykresie\n\nplt.clf()\nplt.plot(X_train, y_train, 'go', label='True data', alpha=0.5)\nplt.plot(X_train, predicted, '--', label='Predictions', alpha=0.5)\nplt.legend(loc='best')\nplt.show()\n\n\n\n\n\n\n\n\n\nimport torch\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\ntorch.manual_seed(1234)\n\n# DANE \nx = torch.linspace(0,10,500).view(-1,1)\ny = torch.sin(x)\ny = y + 0.1*(torch.rand(500).view(-1,1)-0.5)\n\nplt.figure(figsize=(8,4))\nplt.plot(x, torch.sin(x).view(-1,1), color=\"tab:grey\", alpha=0.6, label=\"sin(x)\")\nplt.scatter(x,y, label=\"dane treningowe\")\nplt.xlabel('x')\nplt.ylabel('y')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nclass SinusEstimator(torch.nn.Module):\n\n    def __init__(self, N_INPUT: int, N_OUTPUT: int):\n        super(SinusEstimator,self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(N_INPUT, 64),\n            torch.nn.ReLU(),\n            torch.nn.Linear(64,32),\n            torch.nn.ReLU(),\n            torch.nn.Linear(32,16),\n            torch.nn.Tanh(),\n            torch.nn.Linear(16,N_OUTPUT)\n        )\n\n\n    def forward(self, x):\n        x = self.layers(x)\n        return x\n\nmodel = SinusEstimator(1,1)\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\n\nlosses = []\n\n\ndef callback(model, loss):\n    losses.append(loss.item())\n\n    clear_output(wait=True)\n    prediction = model(x).detach()\n    plt.figure(figsize=(6,2.5))\n    plt.plot(x[:,0].detach(), torch.sin(x)[:,0].detach(), label=\"Exact solution\", color=\"tab:grey\", alpha=0.6)\n    plt.plot(x[:,0].detach(), prediction[:,0], label=\"Classical solution\", color=\"tab:green\")\n    plt.title(f\"Training step {len(losses)}\")\n    plt.legend()\n    plt.show()\n\n    plt.figure(figsize=(6,2.5))\n    plt.title('Lossfn Visualised')\n    plt.plot(losses)\n    plt.show()\n\n\n\n\ndef train(X,Y, model, optimiser, epochs, lossfn, callback = None):\n    for epoch in range(epochs):\n        model.train()\n        prediction = model(X)\n        loss = lossfn(prediction, Y)\n\n        optimiser.zero_grad()\n        loss.backward()\n        optimiser.step()\n        model.eval()\n        if callback != None:\n            callback(model, loss)\n\n\nx_train = x.requires_grad_(False)\n\ntrain(x_train, y, model, optimiser, 300, criterion, callback)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef mse(y, y_pred) -&gt; torch.Tensor:\n    return torch.mean((y-y_pred)**2)\n\ndef special_loss_fn(y, y_pred) -&gt; torch.Tensor:\n    return mse(y, y_pred) + torch.mean((y_pred - torch.sin(x))**2)\n\nmodel2 = SinusEstimator(1,1)\n\nlearning_rate=0.001\noptimiser = torch.optim.Adam(model2.parameters(), lr=learning_rate)\ncriterion = torch.nn.MSELoss()\nlosses = []\n\ntrain(x_train, y, model2, optimiser, 200, special_loss_fn, callback)",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Modele uczenia maszynowego"
    ]
  },
  {
    "objectID": "lectures/kodyw1.html#regresja-logistyczna",
    "href": "lectures/kodyw1.html#regresja-logistyczna",
    "title": "Modele uczenia maszynowego",
    "section": "Regresja logistyczna",
    "text": "Regresja logistyczna\nW przypadku procesu klasyfikacji danych do numerycznego wyniku musimy dodać funkcję aktywacji - sigmoid \\(\\sigma\\), która pozwoli nam wygenerować prawdopodobieństwo otrzymania klasy 1.\nDane wygenerujemy na podstawie pakietu scikit-learn\n\nfrom sklearn.datasets import make_classification\nimport numpy as np\n\n# prepare dataset\nX, y = make_classification(n_samples=10**4, n_features=10 ,random_state=42)\n\n\nimport torch\n\nclass LogisticRegression(torch.nn.Module):\n\n    def __init__(self, inputSize, outputSize):\n        super(LogisticRegression, self).__init__()\n        self.layers = torch.nn.Sequential(\n            torch.nn.Linear(inputSize, outputSize),\n            torch.nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        logits = self.layers(x)\n        return logits\n\nPodobnie jak w przypadku regresji liniowej musimy przetworzyć nasze dane do obiektów torch.\n\nX_train = torch.from_numpy(X.astype(np.float32))\ny_train = torch.from_numpy(y.astype(np.float32))\ny_train = y_train.view(y_train.shape[0], 1)\n\n\nmodel = LogisticRegression(X_train.shape[1], y_train.shape[1])\n\nlearningRate = 0.01\ncriterion = torch.nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n\n# petla uczaca \nnum_epochs = 500\n\nfor epoch in range(num_epochs):\n    # forward pass and loss\n    model.train()\n    y_predicted = model(X_train)\n    loss = criterion(y_predicted, y_train)\n    \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    model.eval()\n\n    if (epoch+1) % 50 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item():.4f}')\n\n\n\nmodel.eval()\nwith torch.no_grad():\n    y_predicted = model(X_train)  # no need to call model.forward()\n    y_predicted_cls = y_predicted.round()   # round off to nearest class\n    acc = y_predicted_cls.eq(y_train).sum() / float(y_train.shape[0])  # accuracy\n    print(f'accuracy = {acc:.4f}')\n    print(f\"predykcja dla wiersza 0:{y_predicted[0]}, wartosc prawdziwa: {y_train[0]}\")\n\nepoch: 50, loss = 0.6307\nepoch: 100, loss = 0.5104\nepoch: 150, loss = 0.4487\nepoch: 200, loss = 0.4120\nepoch: 250, loss = 0.3878\nepoch: 300, loss = 0.3708\nepoch: 350, loss = 0.3581\nepoch: 400, loss = 0.3484\nepoch: 450, loss = 0.3406\nepoch: 500, loss = 0.3343\naccuracy = 0.8830\npredykcja dla wiersza 0:tensor([0.8189]), wartosc prawdziwa: tensor([1.])",
    "crumbs": [
      "Książki",
      "Zajęcia",
      "Modele uczenia maszynowego"
    ]
  }
]